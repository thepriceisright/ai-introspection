{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00314f81",
   "metadata": {},
   "source": [
    "# Reproducing *Emergent Introspective Awareness in LLMs* — End‑to‑End Notebook\n",
    "\n",
    "This notebook runs the four experiments from the paper and recreates the **layer‑wise** plots using the harness you downloaded.\n",
    "\n",
    "**What this notebook does**  \n",
    "1. Unpacks/sets up the harness (or uses an existing `src/` in this working directory).  \n",
    "2. Installs dependencies.  \n",
    "3. Lets you choose a model and runtime (4‑bit/8‑bit/FP16).  \n",
    "4. Runs:\n",
    "   - **Injected thoughts** (layer/strength sweep)\n",
    "   - **Thought vs text**\n",
    "   - **Prefill intention**\n",
    "   - **Intentional control**\n",
    "5. Produces **layer‑wise plots** that match the paper’s figures.\n",
    "\n",
    "**Sources from the paper** (Appendix) this notebook aligns to:\n",
    "- Concept vectors on the *final `:` token* in `Human: Tell me about {word}\\nAssistant:` and subtracting the mean baseline over 100 words (pp. **36–37**).  \n",
    "- Injection windows and experiment prompts (pp. **38–42**, **48–58**).  \n",
    "- Layer‑wise lines and typical peaks (pp. **15**, **21**, **24**, **28–29**).\n",
    "\n",
    "> Note: Exact **numbers** may differ (you’re using open‑source HF models; the paper used Anthropic internal models), but you should recover the **qualitative phenomena** and **layer/strength** trends.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ec3ff3",
   "metadata": {},
   "source": [
    "## 0) Get the code into this working directory\n",
    "\n",
    "- If you have the ZIP handy (e.g., `introspection_repro_with_plots.zip`), run the cell below to unpack it.  \n",
    "- If you already have a `src/` folder here, you can skip unpacking.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d53b2685",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found ./src — skipping unzip.\n"
     ]
    }
   ],
   "source": [
    "#@title Unpack the harness zip (edit ZIP_PATH if needed)\n",
    "import os, zipfile, glob\n",
    "\n",
    "# Change this to the name/path of the ZIP you uploaded to this notebook runtime\n",
    "ZIP_PATH = 'introspection_repro_with_plots.zip'  # or 'introspection_repro.zip'\n",
    "\n",
    "if os.path.isdir('src'):\n",
    "    print('Found ./src — skipping unzip.')\n",
    "elif os.path.exists(ZIP_PATH):\n",
    "    with zipfile.ZipFile(ZIP_PATH, 'r') as z:\n",
    "        z.extractall('.')\n",
    "    print('Unzipped:', ZIP_PATH)\n",
    "    print('Top-level entries:', os.listdir('.')[:10])\n",
    "else:\n",
    "    print('No ./src and no ZIP_PATH found. Please upload the ZIP to this directory or clone the repo here.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f965f1",
   "metadata": {},
   "source": [
    "## 1) Install dependencies\n",
    "\n",
    "- Make sure your CUDA/Torch versions match your GPU drivers.\n",
    "- If you already have Torch installed, you can skip reinstalling it.\n",
    "\n",
    "> **Tip:** For CUDA 12.1+ wheels on Linux: `pip install torch --index-url https://download.pytorch.org/whl/cu121`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a940f3ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/jhvenv/bin/python 25.3\n",
      "Collecting transformers>=4.44.0 (from -r requirements.txt (line 1))\n",
      "  Using cached transformers-4.57.1-py3-none-any.whl.metadata (43 kB)\n",
      "Collecting accelerate>=0.33.0 (from -r requirements.txt (line 2))\n",
      "  Using cached accelerate-1.11.0-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting torch>=2.2.0 (from -r requirements.txt (line 3))\n",
      "  Using cached torch-2.9.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (30 kB)\n",
      "Collecting bitsandbytes>=0.43.0 (from -r requirements.txt (line 4))\n",
      "  Using cached bitsandbytes-0.48.2-py3-none-manylinux_2_24_x86_64.whl.metadata (10 kB)\n",
      "Collecting einops>=0.7.0 (from -r requirements.txt (line 5))\n",
      "  Using cached einops-0.8.1-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /opt/jhvenv/lib/python3.12/site-packages (from -r requirements.txt (line 6)) (2.3.3)\n",
      "Requirement already satisfied: pandas>=2.2.0 in /opt/jhvenv/lib/python3.12/site-packages (from -r requirements.txt (line 7)) (2.3.2)\n",
      "Collecting tqdm>=4.66.0 (from -r requirements.txt (line 8))\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: matplotlib>=3.8.0 in /opt/jhvenv/lib/python3.12/site-packages (from -r requirements.txt (line 9)) (3.10.6)\n",
      "Collecting anthropic>=0.32.0 (from -r requirements.txt (line 10))\n",
      "  Using cached anthropic-0.72.0-py3-none-any.whl.metadata (28 kB)\n",
      "Collecting openai>=1.45.0 (from -r requirements.txt (line 11))\n",
      "  Using cached openai-2.6.1-py3-none-any.whl.metadata (29 kB)\n",
      "Requirement already satisfied: httpx>=0.27.0 in /opt/jhvenv/lib/python3.12/site-packages (from -r requirements.txt (line 12)) (0.28.1)\n",
      "Collecting python-dotenv>=1.0.1 (from -r requirements.txt (line 13))\n",
      "  Using cached python_dotenv-1.2.1-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting tiktoken>=0.12.0 (from -r requirements.txt (line 14))\n",
      "  Using cached tiktoken-0.12.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting protobuf>=6.33.0 (from -r requirements.txt (line 15))\n",
      "  Using cached protobuf-6.33.0-cp39-abi3-manylinux2014_x86_64.whl.metadata (593 bytes)\n",
      "Collecting filelock (from transformers>=4.44.0->-r requirements.txt (line 1))\n",
      "  Using cached filelock-3.20.0-py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting huggingface-hub<1.0,>=0.34.0 (from transformers>=4.44.0->-r requirements.txt (line 1))\n",
      "  Using cached huggingface_hub-0.36.0-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/jhvenv/lib/python3.12/site-packages (from transformers>=4.44.0->-r requirements.txt (line 1)) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/jhvenv/lib/python3.12/site-packages (from transformers>=4.44.0->-r requirements.txt (line 1)) (6.0.2)\n",
      "Collecting regex!=2019.12.17 (from transformers>=4.44.0->-r requirements.txt (line 1))\n",
      "  Using cached regex-2025.10.23-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (40 kB)\n",
      "Requirement already satisfied: requests in /opt/jhvenv/lib/python3.12/site-packages (from transformers>=4.44.0->-r requirements.txt (line 1)) (2.32.5)\n",
      "Collecting tokenizers<=0.23.0,>=0.22.0 (from transformers>=4.44.0->-r requirements.txt (line 1))\n",
      "  Using cached tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers>=4.44.0->-r requirements.txt (line 1))\n",
      "  Using cached safetensors-0.6.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface-hub<1.0,>=0.34.0->transformers>=4.44.0->-r requirements.txt (line 1))\n",
      "  Using cached fsspec-2025.10.0-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/jhvenv/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers>=4.44.0->-r requirements.txt (line 1)) (4.15.0)\n",
      "Collecting hf-xet<2.0.0,>=1.1.3 (from huggingface-hub<1.0,>=0.34.0->transformers>=4.44.0->-r requirements.txt (line 1))\n",
      "  Using cached hf_xet-1.2.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
      "Requirement already satisfied: psutil in /opt/jhvenv/lib/python3.12/site-packages (from accelerate>=0.33.0->-r requirements.txt (line 2)) (7.1.0)\n",
      "Requirement already satisfied: setuptools in /opt/jhvenv/lib/python3.12/site-packages (from torch>=2.2.0->-r requirements.txt (line 3)) (80.9.0)\n",
      "Collecting sympy>=1.13.3 (from torch>=2.2.0->-r requirements.txt (line 3))\n",
      "  Using cached sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting networkx>=2.5.1 (from torch>=2.2.0->-r requirements.txt (line 3))\n",
      "  Using cached networkx-3.5-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: jinja2 in /opt/jhvenv/lib/python3.12/site-packages (from torch>=2.2.0->-r requirements.txt (line 3)) (3.1.6)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.8.93 (from torch>=2.2.0->-r requirements.txt (line 3))\n",
      "  Using cached nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.8.90 (from torch>=2.2.0->-r requirements.txt (line 3))\n",
      "  Using cached nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.8.90 (from torch>=2.2.0->-r requirements.txt (line 3))\n",
      "  Using cached nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.10.2.21 (from torch>=2.2.0->-r requirements.txt (line 3))\n",
      "  Using cached nvidia_cudnn_cu12-9.10.2.21-py3-none-manylinux_2_27_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-cublas-cu12==12.8.4.1 (from torch>=2.2.0->-r requirements.txt (line 3))\n",
      "  Using cached nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cufft-cu12==11.3.3.83 (from torch>=2.2.0->-r requirements.txt (line 3))\n",
      "  Using cached nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.9.90 (from torch>=2.2.0->-r requirements.txt (line 3))\n",
      "  Using cached nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.7.3.90 (from torch>=2.2.0->-r requirements.txt (line 3))\n",
      "  Using cached nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.5.8.93 (from torch>=2.2.0->-r requirements.txt (line 3))\n",
      "  Using cached nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-cusparselt-cu12==0.7.1 (from torch>=2.2.0->-r requirements.txt (line 3))\n",
      "  Using cached nvidia_cusparselt_cu12-0.7.1-py3-none-manylinux2014_x86_64.whl.metadata (7.0 kB)\n",
      "Collecting nvidia-nccl-cu12==2.27.5 (from torch>=2.2.0->-r requirements.txt (line 3))\n",
      "  Using cached nvidia_nccl_cu12-2.27.5-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.0 kB)\n",
      "Collecting nvidia-nvshmem-cu12==3.3.20 (from torch>=2.2.0->-r requirements.txt (line 3))\n",
      "  Using cached nvidia_nvshmem_cu12-3.3.20-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.1 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.8.90 (from torch>=2.2.0->-r requirements.txt (line 3))\n",
      "  Using cached nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-nvjitlink-cu12==12.8.93 (from torch>=2.2.0->-r requirements.txt (line 3))\n",
      "  Using cached nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cufile-cu12==1.13.1.3 (from torch>=2.2.0->-r requirements.txt (line 3))\n",
      "  Using cached nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting triton==3.5.0 (from torch>=2.2.0->-r requirements.txt (line 3))\n",
      "  Using cached triton-3.5.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/jhvenv/lib/python3.12/site-packages (from pandas>=2.2.0->-r requirements.txt (line 7)) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/jhvenv/lib/python3.12/site-packages (from pandas>=2.2.0->-r requirements.txt (line 7)) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/jhvenv/lib/python3.12/site-packages (from pandas>=2.2.0->-r requirements.txt (line 7)) (2025.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/jhvenv/lib/python3.12/site-packages (from matplotlib>=3.8.0->-r requirements.txt (line 9)) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/jhvenv/lib/python3.12/site-packages (from matplotlib>=3.8.0->-r requirements.txt (line 9)) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/jhvenv/lib/python3.12/site-packages (from matplotlib>=3.8.0->-r requirements.txt (line 9)) (4.60.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/jhvenv/lib/python3.12/site-packages (from matplotlib>=3.8.0->-r requirements.txt (line 9)) (1.4.9)\n",
      "Requirement already satisfied: pillow>=8 in /opt/jhvenv/lib/python3.12/site-packages (from matplotlib>=3.8.0->-r requirements.txt (line 9)) (11.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/jhvenv/lib/python3.12/site-packages (from matplotlib>=3.8.0->-r requirements.txt (line 9)) (3.2.5)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /opt/jhvenv/lib/python3.12/site-packages (from anthropic>=0.32.0->-r requirements.txt (line 10)) (4.11.0)\n",
      "Collecting distro<2,>=1.7.0 (from anthropic>=0.32.0->-r requirements.txt (line 10))\n",
      "  Using cached distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting docstring-parser<1,>=0.15 (from anthropic>=0.32.0->-r requirements.txt (line 10))\n",
      "  Using cached docstring_parser-0.17.0-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting jiter<1,>=0.4.0 (from anthropic>=0.32.0->-r requirements.txt (line 10))\n",
      "  Using cached jiter-0.11.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.2 kB)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /opt/jhvenv/lib/python3.12/site-packages (from anthropic>=0.32.0->-r requirements.txt (line 10)) (2.11.9)\n",
      "Requirement already satisfied: sniffio in /opt/jhvenv/lib/python3.12/site-packages (from anthropic>=0.32.0->-r requirements.txt (line 10)) (1.3.1)\n",
      "Requirement already satisfied: certifi in /opt/jhvenv/lib/python3.12/site-packages (from httpx>=0.27.0->-r requirements.txt (line 12)) (2025.8.3)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/jhvenv/lib/python3.12/site-packages (from httpx>=0.27.0->-r requirements.txt (line 12)) (1.0.9)\n",
      "Requirement already satisfied: idna in /opt/jhvenv/lib/python3.12/site-packages (from httpx>=0.27.0->-r requirements.txt (line 12)) (3.10)\n",
      "Requirement already satisfied: h11>=0.16 in /opt/jhvenv/lib/python3.12/site-packages (from httpcore==1.*->httpx>=0.27.0->-r requirements.txt (line 12)) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/jhvenv/lib/python3.12/site-packages (from pydantic<3,>=1.9.0->anthropic>=0.32.0->-r requirements.txt (line 10)) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /opt/jhvenv/lib/python3.12/site-packages (from pydantic<3,>=1.9.0->anthropic>=0.32.0->-r requirements.txt (line 10)) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /opt/jhvenv/lib/python3.12/site-packages (from pydantic<3,>=1.9.0->anthropic>=0.32.0->-r requirements.txt (line 10)) (0.4.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/jhvenv/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas>=2.2.0->-r requirements.txt (line 7)) (1.17.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/jhvenv/lib/python3.12/site-packages (from requests->transformers>=4.44.0->-r requirements.txt (line 1)) (3.4.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/jhvenv/lib/python3.12/site-packages (from requests->transformers>=4.44.0->-r requirements.txt (line 1)) (2.5.0)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy>=1.13.3->torch>=2.2.0->-r requirements.txt (line 3))\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/jhvenv/lib/python3.12/site-packages (from jinja2->torch>=2.2.0->-r requirements.txt (line 3)) (3.0.2)\n",
      "Using cached transformers-4.57.1-py3-none-any.whl (12.0 MB)\n",
      "Using cached huggingface_hub-0.36.0-py3-none-any.whl (566 kB)\n",
      "Using cached hf_xet-1.2.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
      "Using cached tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
      "Using cached accelerate-1.11.0-py3-none-any.whl (375 kB)\n",
      "Using cached torch-2.9.0-cp312-cp312-manylinux_2_28_x86_64.whl (899.7 MB)\n",
      "Using cached nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl (594.3 MB)\n",
      "Using cached nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (10.2 MB)\n",
      "Using cached nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (88.0 MB)\n",
      "Using cached nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (954 kB)\n",
      "Using cached nvidia_cudnn_cu12-9.10.2.21-py3-none-manylinux_2_27_x86_64.whl (706.8 MB)\n",
      "Using cached nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (193.1 MB)\n",
      "Using cached nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.2 MB)\n",
      "Using cached nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl (63.6 MB)\n",
      "Using cached nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl (267.5 MB)\n",
      "Using cached nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (288.2 MB)\n",
      "Using cached nvidia_cusparselt_cu12-0.7.1-py3-none-manylinux2014_x86_64.whl (287.2 MB)\n",
      "Using cached nvidia_nccl_cu12-2.27.5-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (322.3 MB)\n",
      "Using cached nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (39.3 MB)\n",
      "Using cached nvidia_nvshmem_cu12-3.3.20-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (124.7 MB)\n",
      "Using cached nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89 kB)\n",
      "Using cached triton-3.5.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (170.5 MB)\n",
      "Using cached bitsandbytes-0.48.2-py3-none-manylinux_2_24_x86_64.whl (59.4 MB)\n",
      "Using cached einops-0.8.1-py3-none-any.whl (64 kB)\n",
      "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Using cached anthropic-0.72.0-py3-none-any.whl (357 kB)\n",
      "Using cached distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Using cached docstring_parser-0.17.0-py3-none-any.whl (36 kB)\n",
      "Using cached jiter-0.11.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (358 kB)\n",
      "Using cached openai-2.6.1-py3-none-any.whl (1.0 MB)\n",
      "Using cached python_dotenv-1.2.1-py3-none-any.whl (21 kB)\n",
      "Using cached tiktoken-0.12.0-cp312-cp312-manylinux_2_28_x86_64.whl (1.2 MB)\n",
      "Using cached protobuf-6.33.0-cp39-abi3-manylinux2014_x86_64.whl (323 kB)\n",
      "Using cached fsspec-2025.10.0-py3-none-any.whl (200 kB)\n",
      "Using cached networkx-3.5-py3-none-any.whl (2.0 MB)\n",
      "Using cached regex-2025.10.23-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (803 kB)\n",
      "Using cached safetensors-0.6.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (485 kB)\n",
      "Using cached sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Using cached filelock-3.20.0-py3-none-any.whl (16 kB)\n",
      "Installing collected packages: nvidia-cusparselt-cu12, mpmath, triton, tqdm, sympy, safetensors, regex, python-dotenv, protobuf, nvidia-nvtx-cu12, nvidia-nvshmem-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufile-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, networkx, jiter, hf-xet, fsspec, filelock, einops, docstring-parser, distro, tiktoken, nvidia-cusparse-cu12, nvidia-cufft-cu12, nvidia-cudnn-cu12, huggingface-hub, tokenizers, openai, nvidia-cusolver-cu12, anthropic, transformers, torch, bitsandbytes, accelerate\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40/40\u001b[0m [accelerate]celerate]tsandbytes]er-cu12]2]\n",
      "\u001b[1A\u001b[2KSuccessfully installed accelerate-1.11.0 anthropic-0.72.0 bitsandbytes-0.48.2 distro-1.9.0 docstring-parser-0.17.0 einops-0.8.1 filelock-3.20.0 fsspec-2025.10.0 hf-xet-1.2.0 huggingface-hub-0.36.0 jiter-0.11.1 mpmath-1.3.0 networkx-3.5 nvidia-cublas-cu12-12.8.4.1 nvidia-cuda-cupti-cu12-12.8.90 nvidia-cuda-nvrtc-cu12-12.8.93 nvidia-cuda-runtime-cu12-12.8.90 nvidia-cudnn-cu12-9.10.2.21 nvidia-cufft-cu12-11.3.3.83 nvidia-cufile-cu12-1.13.1.3 nvidia-curand-cu12-10.3.9.90 nvidia-cusolver-cu12-11.7.3.90 nvidia-cusparse-cu12-12.5.8.93 nvidia-cusparselt-cu12-0.7.1 nvidia-nccl-cu12-2.27.5 nvidia-nvjitlink-cu12-12.8.93 nvidia-nvshmem-cu12-3.3.20 nvidia-nvtx-cu12-12.8.90 openai-2.6.1 protobuf-6.33.0 python-dotenv-1.2.1 regex-2025.10.23 safetensors-0.6.2 sympy-1.14.0 tiktoken-0.12.0 tokenizers-0.22.1 torch-2.9.0 tqdm-4.67.1 transformers-4.57.1 triton-3.5.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#@title Install Python packages (uncomment Torch line if needed)\n",
    "# !pip install --upgrade pip\n",
    "# If you need a matching Torch build, uncomment ONE of these lines:\n",
    "# !pip install torch --index-url https://download.pytorch.org/whl/cu121   # CUDA 12.1\n",
    "# !pip install torch --index-url https://download.pytorch.org/whl/cu118   # CUDA 11.8\n",
    "\n",
    "# Install the experiment harness deps\n",
    "import sys, pip; print(sys.executable, pip.__version__)\n",
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f705f819",
   "metadata": {},
   "source": [
    "## 2) Wire up API keys for the **LLM grader**\n",
    "\n",
    "The experiments use an **LLM judge** (Anthropic / OpenAI / OpenRouter) with the paper’s grading prompts (Appendix pp. 39–42, 54).  \n",
    "Set any one (or more) of these in the environment for this notebook process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a811a702",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: project .venv directory not found. Create it via `python -m venv .venv`.\n",
      "Loaded variables from .env (existing environment values are preserved).\n",
      "Anthropic: set\n",
      "OpenAI   : set\n",
      "OpenRouter: set\n",
      "HuggingFace: set (HUGGINGFACEHUB_API_TOKEN)\n"
     ]
    }
   ],
   "source": [
    "#@title Load environment variables from the .env file\n",
    "from pathlib import Path\n",
    "import importlib\n",
    "import os\n",
    "import sys\n",
    "\n",
    "\n",
    "def _load_package():\n",
    "    try:\n",
    "        return importlib.import_module(\"introspect_repro\")\n",
    "    except ModuleNotFoundError:\n",
    "        search_roots = [Path.cwd().resolve()]\n",
    "        search_roots += list(search_roots[0].parents)\n",
    "        for root in search_roots:\n",
    "            src_dir = root / \"src\"\n",
    "            if not src_dir.is_dir():\n",
    "                continue\n",
    "            if str(src_dir) not in sys.path:\n",
    "                sys.path.append(str(src_dir))\n",
    "            try:\n",
    "                return importlib.import_module(\"introspect_repro\")\n",
    "            except ModuleNotFoundError:\n",
    "                continue\n",
    "        raise\n",
    "\n",
    "\n",
    "pkg = _load_package()\n",
    "if not hasattr(pkg, \"activate_local_venv\"):\n",
    "    pkg = importlib.reload(pkg)\n",
    "\n",
    "activate_local_venv = getattr(pkg, \"activate_local_venv\")\n",
    "load_project_env = getattr(pkg, \"load_project_env\")\n",
    "get_hf_token = getattr(pkg, \"get_hf_token\", lambda: None)\n",
    "hf_token_env_keys = getattr(pkg, \"HF_TOKEN_ENV_KEYS\", (\n",
    "    \"HUGGINGFACEHUB_API_TOKEN\",\n",
    "    \"HUGGINGFACE_TOKEN\",\n",
    "    \"HF_TOKEN\",\n",
    "    \"HF_API_TOKEN\",\n",
    "))\n",
    "\n",
    "activate_local_venv()\n",
    "load_project_env()\n",
    "\n",
    "project_root = Path(pkg.__file__).resolve().parent.parent\n",
    "project_venv = project_root / \".venv\"\n",
    "interpreter_path = Path(sys.executable)\n",
    "if project_venv.exists():\n",
    "    if project_venv in interpreter_path.parents:\n",
    "        print(f\"Using interpreter: {interpreter_path}\")\n",
    "    else:\n",
    "        print(f\"Warning: kernel interpreter {interpreter_path} is outside .venv; added .venv site-packages to sys.path.\")\n",
    "else:\n",
    "    print(\"Warning: project .venv directory not found. Create it via `python -m venv .venv`.\")\n",
    "\n",
    "print(\"Loaded variables from .env (existing environment values are preserved).\")\n",
    "status_labels = (\n",
    "    (\"ANTHROPIC_API_KEY\", \"Anthropic\"),\n",
    "    (\"OPENAI_API_KEY\", \"OpenAI   \"),\n",
    "    (\"OPENROUTER_API_KEY\", \"OpenRouter\"),\n",
    ")\n",
    "for key, label in status_labels:\n",
    "    print(f\"{label}: {'set' if os.environ.get(key) else 'not set'}\")\n",
    "\n",
    "hf_token = get_hf_token()\n",
    "if hf_token:\n",
    "    first_key = next((key for key in hf_token_env_keys if os.environ.get(key)), None)\n",
    "    if first_key:\n",
    "        print(f\"HuggingFace: set ({first_key})\")\n",
    "    else:\n",
    "        print(\"HuggingFace: set\")\n",
    "else:\n",
    "    print(\"HuggingFace: not set\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "934cbe06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anthropic: set\n",
      "OpenAI   : set\n",
      "OpenRouter: set\n"
     ]
    }
   ],
   "source": [
    "#@title Set API keys for grading (fill values or leave to inherit from your environment)\n",
    "import os\n",
    "\n",
    "# Fill in only the ones you plan to use. Leaving blank keeps the existing environment value.\n",
    "ANTHROPIC = \"\"  # e.g., \"sk-ant-...\"\n",
    "OPENAI = \"\"     # e.g., \"sk-...\"\n",
    "OPENROUTER = \"\" # e.g., \"or-...\"\n",
    "\n",
    "if ANTHROPIC: os.environ['ANTHROPIC_API_KEY'] = ANTHROPIC\n",
    "if OPENAI:    os.environ['OPENAI_API_KEY'] = OPENAI\n",
    "if OPENROUTER:os.environ['OPENROUTER_API_KEY'] = OPENROUTER\n",
    "\n",
    "print(\"Anthropic:\", \"set\" if os.environ.get(\"ANTHROPIC_API_KEY\") else \"not set\")\n",
    "print(\"OpenAI   :\", \"set\" if os.environ.get(\"OPENAI_API_KEY\") else \"not set\")\n",
    "print(\"OpenRouter:\", \"set\" if os.environ.get(\"OPENROUTER_API_KEY\") else \"not set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de38bde",
   "metadata": {},
   "source": [
    "## 3) Configure model and runtime\n",
    "\n",
    "- **HF model**: any decoder‑only chat/instruct model with access to internals (e.g., Llama‑3‑8B‑Instruct, Qwen‑2.5‑7B/14B, Mixtral).  \n",
    "- **Precision**: FP16 if you have VRAM; otherwise 8‑bit/4‑bit.  \n",
    "- **Judge**: pick provider/model for grading.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83fa7c90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: meta-llama/Meta-Llama-3-8B\n",
      "Judge: openai / gpt-5-mini\n"
     ]
    }
   ],
   "source": [
    "#@title Choose your HF model and judge\n",
    "HF_MODEL = \"meta-llama/Meta-Llama-3-8B\"  #@param {type:\"string\"}\n",
    "LOAD_IN_4BIT = True                                #@param {type:\"boolean\"}\n",
    "LOAD_IN_8BIT = False                               #@param {type:\"boolean\"}\n",
    "DTYPE = \"bfloat16\"                                  #@param [\"bfloat16\",\"float16\",\"auto\"]\n",
    "\n",
    "JUDGE_PROVIDER = \"openai\"                           #@param [\"openai\",\"anthropic\",\"openrouter\"]\n",
    "JUDGE_MODEL = \"gpt-5-mini\"                         #@param {type:\"string\"}\n",
    "\n",
    "N_TRIALS_INJECTED = 30                              #@param {type:\"integer\"}\n",
    "N_TRIALS_TVT = 50                                   #@param {type:\"integer\"}\n",
    "N_TRIALS_PREFILL = 50                               #@param {type:\"integer\"}\n",
    "N_TRIALS_INT_CTRL = 16                              #@param {type:\"integer\"}\n",
    "\n",
    "print(\"Model:\", HF_MODEL)\n",
    "print(\"Judge:\", JUDGE_PROVIDER, \"/\", JUDGE_MODEL)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd64e83",
   "metadata": {},
   "source": [
    "### Add the harness to `sys.path` and compute a good default layer\n",
    "\n",
    "We avoid loading full weights just to count layers by reading the config.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "612e1436",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "347cbb3ad57e47de9eabc861266862d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/654 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_hidden_layers: 32 |  ~2/3 layer: 20\n"
     ]
    }
   ],
   "source": [
    "#@title Prepare imports and compute a 2/3‑depth layer guess\n",
    "import os, sys, glob, math, subprocess, shlex, json, pathlib, time\n",
    "from IPython.display import display, Image\n",
    "\n",
    "# Add ./src to import path\n",
    "SRC_CANDIDATES = [\"./src\", \"../src\", \"/workspace/src\"]\n",
    "for c in SRC_CANDIDATES:\n",
    "    if os.path.isdir(c) and os.path.abspath(c) not in sys.path:\n",
    "        sys.path.insert(0, os.path.abspath(c))\n",
    "\n",
    "# Light‑weight: use AutoConfig to count layers\n",
    "from transformers import AutoConfig\n",
    "cfg = AutoConfig.from_pretrained(HF_MODEL)\n",
    "NUM_LAYERS = getattr(cfg, \"num_hidden_layers\", None) or getattr(cfg, \"n_layer\", None)\n",
    "LAYER_2_3 = int(round(0.66 * (NUM_LAYERS - 1))) if NUM_LAYERS else None\n",
    "print(\"num_hidden_layers:\", NUM_LAYERS, \"|  ~2/3 layer:\", LAYER_2_3)\n",
    "\n",
    "# Convenience to run a python module from this kernel\n",
    "def run_module(modname, args_list):\n",
    "    cmd = [sys.executable, \"-m\", modname] + list(map(str, args_list))\n",
    "    print(\">>>\", \" \".join(cmd))\n",
    "    subprocess.run(cmd, check=True)\n",
    "\n",
    "def latest_run_dir(exp_name):\n",
    "    cands = glob.glob(os.path.join(\"runs\", \"*\", exp_name))\n",
    "    return max(cands, key=os.path.getmtime) if cands else None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e7e7212",
   "metadata": {},
   "source": [
    "## 4) Experiment A — **Injected thoughts** (layer & strength sweeps)\n",
    "\n",
    "Protocol (Appendix pp. 36–42): compute concept vectors on the `Assistant:` colon token, subtract baseline mean (100 words), then inject on the Assistant turn of the **“injected thoughts”** prompt and judge **coherence ∧ affirmative ∧ correct identification (before saying the word)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2ac2d365",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> /opt/jhvenv/bin/python -m src.introspect_repro.experiments.injected_thoughts --model meta-llama/Meta-Llama-3-8B --judge-provider openai --judge-model gpt-5-mini --n-trials 30 --sweep-layers 0.6 0.66 0.7 0.75 0.8 --strengths 1 2 4 --load-in-4bit --dtype bfloat16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
      "Fetching 4 files: 100%|██████████| 4/4 [00:51<00:00, 12.80s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:11<00:00,  2.90s/it]\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Traceback (most recent call last):\n",
      "  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"/home/keithvertrees/Projects/ai-introspection/src/introspect_repro/experiments/injected_thoughts.py\", line 69, in <module>\n",
      "    main()\n",
      "  File \"/home/keithvertrees/Projects/ai-introspection/src/introspect_repro/experiments/injected_thoughts.py\", line 59, in main\n",
      "    coh = judge.grade_coherence(prompt, response)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/keithvertrees/Projects/ai-introspection/src/introspect_repro/judges.py\", line 148, in grade_coherence\n",
      "    return self._ask(COHERENCE_PROMPT.format(prompt=prompt, response=response))\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/keithvertrees/Projects/ai-introspection/src/introspect_repro/judges.py\", line 143, in _ask\n",
      "    out = _call_llm(self.cfg.provider, self.cfg.model, prompt,\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/keithvertrees/Projects/ai-introspection/src/introspect_repro/judges.py\", line 124, in _call_llm\n",
      "    return _openai_chat(messages, model, temperature, max_tokens)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/keithvertrees/Projects/ai-introspection/src/introspect_repro/judges.py\", line 99, in _openai_chat\n",
      "    resp = client.chat.completions.create(model=model, messages=messages,\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/jhvenv/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 286, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/jhvenv/lib/python3.12/site-packages/openai/resources/chat/completions/completions.py\", line 1156, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"/opt/jhvenv/lib/python3.12/site-packages/openai/_base_client.py\", line 1259, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/jhvenv/lib/python3.12/site-packages/openai/_base_client.py\", line 1047, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.BadRequestError: Error code: 400 - {'error': {'message': \"Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.\", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}\n"
     ]
    },
    {
     "ename": "CalledProcessError",
     "evalue": "Command '['/opt/jhvenv/bin/python', '-m', 'src.introspect_repro.experiments.injected_thoughts', '--model', 'meta-llama/Meta-Llama-3-8B', '--judge-provider', 'openai', '--judge-model', 'gpt-5-mini', '--n-trials', '30', '--sweep-layers', '0.6', '0.66', '0.7', '0.75', '0.8', '--strengths', '1', '2', '4', '--load-in-4bit', '--dtype', 'bfloat16']' returned non-zero exit status 1.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mCalledProcessError\u001b[39m                        Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m LOAD_IN_8BIT: args.append(\u001b[33m\"\u001b[39m\u001b[33m--load-in-8bit\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m DTYPE \u001b[38;5;129;01mand\u001b[39;00m DTYPE != \u001b[33m\"\u001b[39m\u001b[33mauto\u001b[39m\u001b[33m\"\u001b[39m: args += [\u001b[33m\"\u001b[39m\u001b[33m--dtype\u001b[39m\u001b[33m\"\u001b[39m, DTYPE]\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m \u001b[43mrun_module\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msrc.introspect_repro.experiments.injected_thoughts\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 22\u001b[39m, in \u001b[36mrun_module\u001b[39m\u001b[34m(modname, args_list)\u001b[39m\n\u001b[32m     20\u001b[39m cmd = [sys.executable, \u001b[33m\"\u001b[39m\u001b[33m-m\u001b[39m\u001b[33m\"\u001b[39m, modname] + \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28mstr\u001b[39m, args_list))\n\u001b[32m     21\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m>>>\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m.join(cmd))\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m \u001b[43msubprocess\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcmd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/subprocess.py:571\u001b[39m, in \u001b[36mrun\u001b[39m\u001b[34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[39m\n\u001b[32m    569\u001b[39m     retcode = process.poll()\n\u001b[32m    570\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m check \u001b[38;5;129;01mand\u001b[39;00m retcode:\n\u001b[32m--> \u001b[39m\u001b[32m571\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m CalledProcessError(retcode, process.args,\n\u001b[32m    572\u001b[39m                                  output=stdout, stderr=stderr)\n\u001b[32m    573\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m CompletedProcess(process.args, retcode, stdout, stderr)\n",
      "\u001b[31mCalledProcessError\u001b[39m: Command '['/opt/jhvenv/bin/python', '-m', 'src.introspect_repro.experiments.injected_thoughts', '--model', 'meta-llama/Meta-Llama-3-8B', '--judge-provider', 'openai', '--judge-model', 'gpt-5-mini', '--n-trials', '30', '--sweep-layers', '0.6', '0.66', '0.7', '0.75', '0.8', '--strengths', '1', '2', '4', '--load-in-4bit', '--dtype', 'bfloat16']' returned non-zero exit status 1."
     ]
    }
   ],
   "source": [
    "#@title Run injected thoughts (layer/strength sweep)\n",
    "LAYER_FRACTIONS = [0.60, 0.66, 0.70, 0.75, 0.80]  #@param\n",
    "STRENGTHS = [1,2,4]                               #@param\n",
    "\n",
    "args = [\n",
    "    \"--model\", HF_MODEL,\n",
    "    \"--judge-provider\", JUDGE_PROVIDER,\n",
    "    \"--judge-model\", JUDGE_MODEL,\n",
    "    \"--n-trials\", N_TRIALS_INJECTED,\n",
    "    \"--sweep-layers\", *LAYER_FRACTIONS,\n",
    "    \"--strengths\", *STRENGTHS\n",
    "]\n",
    "if LOAD_IN_4BIT: args.append(\"--load-in-4bit\")\n",
    "if LOAD_IN_8BIT: args.append(\"--load-in-8bit\")\n",
    "if DTYPE and DTYPE != \"auto\": args += [\"--dtype\", DTYPE]\n",
    "\n",
    "run_module(\"src.introspect_repro.experiments.injected_thoughts\", args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9904dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Plot injected thoughts: layer‑wise lines\n",
    "INJECTED_RUN = latest_run_dir(\"injected_thoughts\")\n",
    "print(\"Run dir:\", INJECTED_RUN)\n",
    "\n",
    "# Pick one strength to match the paper’s typical layer‑wise figure (strength≈2)\n",
    "PLOT_STRENGTH = 2\n",
    "png = os.path.join(INJECTED_RUN, f\"layerwise_strength{PLOT_STRENGTH}.png\")\n",
    "\n",
    "run_module(\"src.introspect_repro.plotting.plot_injected_thoughts\",\n",
    "           [\"--run-dir\", INJECTED_RUN, \"--strength\", PLOT_STRENGTH, \"--save\", png])\n",
    "\n",
    "display(Image(filename=png))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb76ecb",
   "metadata": {},
   "source": [
    "## 5) Experiment B — **Thought vs Text**\n",
    "\n",
    "Protocol (pp. 20–22; Appendix pp. 48–51): inject an unrelated word over the sentence tokens, then ask the model (a) **what word it thinks about** (judge “YES”), and (b) to **repeat the sentence exactly** (string match)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98fd4dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Run thought vs text at a single layer\n",
    "TVT_LAYER = LAYER_2_3 if LAYER_2_3 is not None else 12  #@param {type:\"integer\"}\n",
    "TVT_STRENGTH = 2                                        #@param {type:\"integer\"}\n",
    "\n",
    "args = [\n",
    "    \"--model\", HF_MODEL,\n",
    "    \"--layer\", TVT_LAYER,\n",
    "    \"--strength\", TVT_STRENGTH,\n",
    "    \"--n-trials\", N_TRIALS_TVT,\n",
    "    \"--judge-provider\", JUDGE_PROVIDER,\n",
    "    \"--judge-model\", JUDGE_MODEL\n",
    "]\n",
    "if LOAD_IN_4BIT: args.append(\"--load-in-4bit\")\n",
    "if LOAD_IN_8BIT: args.append(\"--load-in-8bit\")\n",
    "if DTYPE and DTYPE != \"auto\": args += [\"--dtype\", DTYPE]\n",
    "\n",
    "run_module(\"introspect_repro.experiments.thought_vs_text\", args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f340892c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Plot thought vs text — layer‑wise lines (use latest run folder)\n",
    "TVT_RUN = latest_run_dir(\"thought_vs_text\")\n",
    "png = os.path.join(TVT_RUN, f\"tvt_layerwise_strength{TVT_STRENGTH}.png\")\n",
    "\n",
    "run_module(\"introspect_repro.plotting.plot_thought_vs_text\",\n",
    "           [\"--run-dir\", TVT_RUN, \"--strength\", TVT_STRENGTH, \"--save\", png])\n",
    "\n",
    "from IPython.display import Image, display\n",
    "display(Image(filename=png))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a23452",
   "metadata": {},
   "source": [
    "## 6) Experiment C — **Prefill intention** (apology rate)\n",
    "\n",
    "Protocol (pp. 22–25; Appendix pp. 53–55): prefill an **unrelated word**, ask whether it was intended; then **retroactively inject** the concept corresponding to the prefilled word **prior** to the prefill. Judge **apology vs intended**; plot **apology rate** vs layer (lower is better). Peak is typically **earlier** than the ~2/3 layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca1ea5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Run prefill intention\n",
    "PREFILL_LAYER = max(0, int(round(0.55 * (NUM_LAYERS-1)))) if NUM_LAYERS else 12  #@param {type:\"integer\"}\n",
    "PREFILL_STRENGTH = 4                                                               #@param {type:\"integer\"}\n",
    "\n",
    "args = [\n",
    "    \"--model\", HF_MODEL,\n",
    "    \"--layer\", PREFILL_LAYER,\n",
    "    \"--strength\", PREFILL_STRENGTH,\n",
    "    \"--n-trials\", N_TRIALS_PREFILL,\n",
    "    \"--judge-provider\", JUDGE_PROVIDER,\n",
    "    \"--judge-model\", JUDGE_MODEL\n",
    "]\n",
    "if LOAD_IN_4BIT: args.append(\"--load-in-4bit\")\n",
    "if LOAD_IN_8BIT: args.append(\"--load-in-8bit\")\n",
    "if DTYPE and DTYPE != \"auto\": args += [\"--dtype\", DTYPE]\n",
    "\n",
    "run_module(\"introspect_repro.experiments.prefill_intention\", args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb50b4fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Plot prefill intention — apology rate vs layer\n",
    "PREFILL_RUN = latest_run_dir(\"prefill_intention\")\n",
    "png = os.path.join(PREFILL_RUN, f\"prefill_apology_layerwise_strength{PREFILL_STRENGTH}.png\")\n",
    "\n",
    "run_module(\"introspect_repro.plotting.plot_prefill_intention\",\n",
    "           [\"--run-dir\", PREFILL_RUN, \"--strength\", PREFILL_STRENGTH, \"--save\", png])\n",
    "\n",
    "from IPython.display import Image, display\n",
    "display(Image(filename=png))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e75b31e4",
   "metadata": {},
   "source": [
    "## 7) Experiment D — **Intentional control** (think vs don’t think)\n",
    "\n",
    "Protocol (pp. 26–29; Appendix pp. 56–58): instruct **“Think about {word}”** versus **“Don’t think about {word}”**, record residuals during generation, and plot mean **cosine(model activations, concept vector)** vs layer for both cases.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d437f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Run intentional control across several layers\n",
    "LAYERS_TO_RUN = [0.50, 0.60, 0.66, 0.75, 0.85]  # as fractions of depth\n",
    "layers = [max(0, min(NUM_LAYERS-1, int(round(fr*(NUM_LAYERS-1))))) for fr in LAYERS_TO_RUN]\n",
    "\n",
    "for L in layers:\n",
    "    args = [\n",
    "        \"--model\", HF_MODEL,\n",
    "        \"--layer\", L,\n",
    "        \"--n-trials\", N_TRIALS_INT_CTRL\n",
    "    ]\n",
    "    if LOAD_IN_4BIT: args.append(\"--load-in-4bit\")\n",
    "    if LOAD_IN_8BIT: args.append(\"--load-in-8bit\")\n",
    "    if DTYPE and DTYPE != \"auto\": args += [\"--dtype\", DTYPE]\n",
    "\n",
    "    print(\"\\n=== Intentional control at layer\", L, \"===\")\n",
    "    run_module(\"introspect_repro.experiments.intentional_control\", args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "949202db",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Plot intentional control — mean cosine vs layer\n",
    "INTC_RUN = latest_run_dir(\"intentional_control\")\n",
    "png = os.path.join(INTC_RUN, \"intent_control_layerwise.png\")\n",
    "\n",
    "run_module(\"introspect_repro.plotting.plot_intent_control\",\n",
    "           [\"--run-dir\", INTC_RUN, \"--save\", png])\n",
    "\n",
    "from IPython.display import Image, display\n",
    "display(Image(filename=png))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1fc0d32",
   "metadata": {},
   "source": [
    "## 8) (Optional) Summarize injected‑thoughts metrics as a table\n",
    "\n",
    "The “awareness” column = **coherence ∧ affirmative ∧ correct identification**, matching the Appendix grader criteria (pp. 39–42).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "317fdeca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Build a summary table\n",
    "import json, os, glob\n",
    "import pandas as pd\n",
    "\n",
    "from introspect_repro.plotting.utils import load_results\n",
    "\n",
    "INJ = latest_run_dir(\"injected_thoughts\")\n",
    "rows = []\n",
    "for (layer, strength, j, f) in load_results(INJ):\n",
    "    trials = j[\"trials\"]\n",
    "    coh = [t.get(\"coherence\") for t in trials]\n",
    "    aff = [t.get(\"affirmative\") for t in trials]\n",
    "    cor = [t.get(\"correct_identification\") for t in trials]\n",
    "    aware = [ (t.get(\"coherence\") and t.get(\"affirmative\") and t.get(\"correct_identification\")) for t in trials ]\n",
    "    rows.append(dict(layer=layer, strength=strength,\n",
    "                     n=len(trials),\n",
    "                     coherence=sum(1 for x in coh if x)/len(coh) if coh else 0.0,\n",
    "                     affirmative=sum(1 for x in aff if x)/len(aff) if aff else 0.0,\n",
    "                     correct_id=sum(1 for x in cor if x)/len(cor) if cor else 0.0,\n",
    "                     awareness=sum(1 for x in aware if x)/len(aware) if aware else 0.0))\n",
    "df = pd.DataFrame(rows).sort_values([\"strength\",\"layer\"])\n",
    "df.style.format({c:\"{:.3f}\" for c in [\"coherence\",\"affirmative\",\"correct_id\",\"awareness\"]})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91477715",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Tips & troubleshooting\n",
    "\n",
    "- **VRAM errors** → switch to `LOAD_IN_4BIT=True`, set `DTYPE=\"bfloat16\"`, reduce `N_TRIALS_*`.  \n",
    "- **Judge time/cost** → use OpenAI `gpt-4o-mini` or Anthropic `haiku`-class as judges for quick passes.  \n",
    "- **Peaks by layer** → injected‑thoughts & thought‑vs‑text often peak near **~2/3 depth**; prefill sometimes peaks **earlier** (p. 24).  \n",
    "- **High strengths** → may cause incoherence/“brain damage” (pp. 13–14); stick to {1,2,4,8}.\n",
    "\n",
    "**Safety**: The Appendix includes a control prompt mentioning *donating to terrorist organizations*; the harness leaves that variant out by default.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
