{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# Reproducing *Emergent Introspective Awareness in LLMs* — End‑to‑End Notebook\n\nThis notebook runs the four experiments from the paper and recreates the **layer‑wise** plots using the harness you downloaded.\n\n**What this notebook does**  \n1. Unpacks/sets up the harness (or uses an existing `src/` in this working directory).  \n2. Installs dependencies.  \n3. Lets you choose a model and runtime (4‑bit/8‑bit/FP16).  \n4. Runs:\n   - **Injected thoughts** (layer/strength sweep)\n   - **Thought vs text**\n   - **Prefill intention**\n   - **Intentional control**\n5. Produces **layer‑wise plots** that match the paper’s figures.\n\n**Sources from the paper** (Appendix) this notebook aligns to:\n- Concept vectors on the *final `:` token* in `Human: Tell me about {word}\\nAssistant:` and subtracting the mean baseline over 100 words (pp. **36–37**).  \n- Injection windows and experiment prompts (pp. **38–42**, **48–58**).  \n- Layer‑wise lines and typical peaks (pp. **15**, **21**, **24**, **28–29**).\n\n> Note: Exact **numbers** may differ (you’re using open‑source HF models; the paper used Anthropic internal models), but you should recover the **qualitative phenomena** and **layer/strength** trends.\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 0) Get the code into this working directory\n\n- If you have the ZIP handy (e.g., `introspection_repro_with_plots.zip`), run the cell below to unpack it.  \n- If you already have a `src/` folder here, you can skip unpacking.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "#@title Unpack the harness zip (edit ZIP_PATH if needed)\nimport os, zipfile, glob\n\n# Change this to the name/path of the ZIP you uploaded to this notebook runtime\nZIP_PATH = 'introspection_repro_with_plots.zip'  # or 'introspection_repro.zip'\n\nif os.path.isdir('src'):\n    print('Found ./src — skipping unzip.')\nelif os.path.exists(ZIP_PATH):\n    with zipfile.ZipFile(ZIP_PATH, 'r') as z:\n        z.extractall('.')\n    print('Unzipped:', ZIP_PATH)\n    print('Top-level entries:', os.listdir('.')[:10])\nelse:\n    print('No ./src and no ZIP_PATH found. Please upload the ZIP to this directory or clone the repo here.')",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 1) Install dependencies\n\n- Make sure your CUDA/Torch versions match your GPU drivers.\n- If you already have Torch installed, you can skip reinstalling it.\n\n> **Tip:** For CUDA 12.1+ wheels on Linux: `pip install torch --index-url https://download.pytorch.org/whl/cu121`\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "#@title Install Python packages (uncomment Torch line if needed)\n# !pip install --upgrade pip\n# If you need a matching Torch build, uncomment ONE of these lines:\n# !pip install torch --index-url https://download.pytorch.org/whl/cu121   # CUDA 12.1\n# !pip install torch --index-url https://download.pytorch.org/whl/cu118   # CUDA 11.8\n\n# Install the experiment harness deps\n!pip install -r requirements.txt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 2) Wire up API keys for the **LLM grader**\n\nThe experiments use an **LLM judge** (Anthropic / OpenAI / OpenRouter) with the paper’s grading prompts (Appendix pp. 39–42, 54).  \nSet any one (or more) of these in the environment for this notebook process.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "#@title Set API keys for grading (fill values or leave to inherit from your environment)\nimport os\n\n# Fill in only the ones you plan to use. Leaving blank keeps the existing environment value.\nANTHROPIC = \"\"  # e.g., \"sk-ant-...\"\nOPENAI = \"\"     # e.g., \"sk-...\"\nOPENROUTER = \"\" # e.g., \"or-...\"\n\nif ANTHROPIC: os.environ['ANTHROPIC_API_KEY'] = ANTHROPIC\nif OPENAI:    os.environ['OPENAI_API_KEY'] = OPENAI\nif OPENROUTER:os.environ['OPENROUTER_API_KEY'] = OPENROUTER\n\nprint(\"Anthropic:\", \"set\" if os.environ.get(\"ANTHROPIC_API_KEY\") else \"not set\")\nprint(\"OpenAI   :\", \"set\" if os.environ.get(\"OPENAI_API_KEY\") else \"not set\")\nprint(\"OpenRouter:\", \"set\" if os.environ.get(\"OPENROUTER_API_KEY\") else \"not set\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 3) Configure model and runtime\n\n- **HF model**: any decoder‑only chat/instruct model with access to internals (e.g., Llama‑3‑8B‑Instruct, Qwen‑2.5‑7B/14B, Mixtral).  \n- **Precision**: FP16 if you have VRAM; otherwise 8‑bit/4‑bit.  \n- **Judge**: pick provider/model for grading.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "#@title Choose your HF model and judge\nHF_MODEL = \"meta-llama/Meta-Llama-3-8B-Instruct\"  #@param {type:\"string\"}\nLOAD_IN_4BIT = True                                #@param {type:\"boolean\"}\nLOAD_IN_8BIT = False                               #@param {type:\"boolean\"}\nDTYPE = \"bfloat16\"                                  #@param [\"bfloat16\",\"float16\",\"auto\"]\n\nJUDGE_PROVIDER = \"openai\"                           #@param [\"openai\",\"anthropic\",\"openrouter\"]\nJUDGE_MODEL = \"gpt-4o-mini\"                         #@param {type:\"string\"}\n\nN_TRIALS_INJECTED = 30                              #@param {type:\"integer\"}\nN_TRIALS_TVT = 50                                   #@param {type:\"integer\"}\nN_TRIALS_PREFILL = 50                               #@param {type:\"integer\"}\nN_TRIALS_INT_CTRL = 16                              #@param {type:\"integer\"}\n\nprint(\"Model:\", HF_MODEL)\nprint(\"Judge:\", JUDGE_PROVIDER, \"/\", JUDGE_MODEL)\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Add the harness to `sys.path` and compute a good default layer\n\nWe avoid loading full weights just to count layers by reading the config.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "#@title Prepare imports and compute a 2/3‑depth layer guess\nimport os, sys, glob, math, subprocess, shlex, json, pathlib, time\nfrom IPython.display import display, Image\n\n# Add ./src to import path\nSRC_CANDIDATES = [\"./src\", \"../src\", \"/workspace/src\"]\nfor c in SRC_CANDIDATES:\n    if os.path.isdir(c) and os.path.abspath(c) not in sys.path:\n        sys.path.insert(0, os.path.abspath(c))\n\n# Light‑weight: use AutoConfig to count layers\nfrom transformers import AutoConfig\ncfg = AutoConfig.from_pretrained(HF_MODEL)\nNUM_LAYERS = getattr(cfg, \"num_hidden_layers\", None) or getattr(cfg, \"n_layer\", None)\nLAYER_2_3 = int(round(0.66 * (NUM_LAYERS - 1))) if NUM_LAYERS else None\nprint(\"num_hidden_layers:\", NUM_LAYERS, \"|  ~2/3 layer:\", LAYER_2_3)\n\n# Convenience to run a python module from this kernel\ndef run_module(modname, args_list):\n    cmd = [sys.executable, \"-m\", modname] + list(map(str, args_list))\n    print(\">>>\", \" \".join(cmd))\n    subprocess.run(cmd, check=True)\n\ndef latest_run_dir(exp_name):\n    cands = glob.glob(os.path.join(\"runs\", \"*\", exp_name))\n    return max(cands, key=os.path.getmtime) if cands else None\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 4) Experiment A — **Injected thoughts** (layer & strength sweeps)\n\nProtocol (Appendix pp. 36–42): compute concept vectors on the `Assistant:` colon token, subtract baseline mean (100 words), then inject on the Assistant turn of the **“injected thoughts”** prompt and judge **coherence ∧ affirmative ∧ correct identification (before saying the word)**."
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "#@title Run injected thoughts (layer/strength sweep)\nLAYER_FRACTIONS = [0.60, 0.66, 0.70, 0.75, 0.80]  #@param\nSTRENGTHS = [1,2,4]                               #@param\n\nargs = [\n    \"--model\", HF_MODEL,\n    \"--judge-provider\", JUDGE_PROVIDER,\n    \"--judge-model\", JUDGE_MODEL,\n    \"--n-trials\", N_TRIALS_INJECTED,\n    \"--sweep-layers\", *LAYER_FRACTIONS,\n    \"--strengths\", *STRENGTHS\n]\nif LOAD_IN_4BIT: args.append(\"--load-in-4bit\")\nif LOAD_IN_8BIT: args.append(\"--load-in-8bit\")\nif DTYPE and DTYPE != \"auto\": args += [\"--dtype\", DTYPE]\n\nrun_module(\"introspect_repro.experiments.injected_thoughts\", args)",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "#@title Plot injected thoughts: layer‑wise lines\nINJECTED_RUN = latest_run_dir(\"injected_thoughts\")\nprint(\"Run dir:\", INJECTED_RUN)\n\n# Pick one strength to match the paper’s typical layer‑wise figure (strength≈2)\nPLOT_STRENGTH = 2\npng = os.path.join(INJECTED_RUN, f\"layerwise_strength{PLOT_STRENGTH}.png\")\n\nrun_module(\"introspect_repro.plotting.plot_injected_thoughts\",\n           [\"--run-dir\", INJECTED_RUN, \"--strength\", PLOT_STRENGTH, \"--save\", png])\n\ndisplay(Image(filename=png))",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 5) Experiment B — **Thought vs Text**\n\nProtocol (pp. 20–22; Appendix pp. 48–51): inject an unrelated word over the sentence tokens, then ask the model (a) **what word it thinks about** (judge “YES”), and (b) to **repeat the sentence exactly** (string match)."
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "#@title Run thought vs text at a single layer\nTVT_LAYER = LAYER_2_3 if LAYER_2_3 is not None else 12  #@param {type:\"integer\"}\nTVT_STRENGTH = 2                                        #@param {type:\"integer\"}\n\nargs = [\n    \"--model\", HF_MODEL,\n    \"--layer\", TVT_LAYER,\n    \"--strength\", TVT_STRENGTH,\n    \"--n-trials\", N_TRIALS_TVT,\n    \"--judge-provider\", JUDGE_PROVIDER,\n    \"--judge-model\", JUDGE_MODEL\n]\nif LOAD_IN_4BIT: args.append(\"--load-in-4bit\")\nif LOAD_IN_8BIT: args.append(\"--load-in-8bit\")\nif DTYPE and DTYPE != \"auto\": args += [\"--dtype\", DTYPE]\n\nrun_module(\"introspect_repro.experiments.thought_vs_text\", args)",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "#@title Plot thought vs text — layer‑wise lines (use latest run folder)\nTVT_RUN = latest_run_dir(\"thought_vs_text\")\npng = os.path.join(TVT_RUN, f\"tvt_layerwise_strength{TVT_STRENGTH}.png\")\n\nrun_module(\"introspect_repro.plotting.plot_thought_vs_text\",\n           [\"--run-dir\", TVT_RUN, \"--strength\", TVT_STRENGTH, \"--save\", png])\n\nfrom IPython.display import Image, display\ndisplay(Image(filename=png))",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 6) Experiment C — **Prefill intention** (apology rate)\n\nProtocol (pp. 22–25; Appendix pp. 53–55): prefill an **unrelated word**, ask whether it was intended; then **retroactively inject** the concept corresponding to the prefilled word **prior** to the prefill. Judge **apology vs intended**; plot **apology rate** vs layer (lower is better). Peak is typically **earlier** than the ~2/3 layer."
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "#@title Run prefill intention\nPREFILL_LAYER = max(0, int(round(0.55 * (NUM_LAYERS-1)))) if NUM_LAYERS else 12  #@param {type:\"integer\"}\nPREFILL_STRENGTH = 4                                                               #@param {type:\"integer\"}\n\nargs = [\n    \"--model\", HF_MODEL,\n    \"--layer\", PREFILL_LAYER,\n    \"--strength\", PREFILL_STRENGTH,\n    \"--n-trials\", N_TRIALS_PREFILL,\n    \"--judge-provider\", JUDGE_PROVIDER,\n    \"--judge-model\", JUDGE_MODEL\n]\nif LOAD_IN_4BIT: args.append(\"--load-in-4bit\")\nif LOAD_IN_8BIT: args.append(\"--load-in-8bit\")\nif DTYPE and DTYPE != \"auto\": args += [\"--dtype\", DTYPE]\n\nrun_module(\"introspect_repro.experiments.prefill_intention\", args)",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "#@title Plot prefill intention — apology rate vs layer\nPREFILL_RUN = latest_run_dir(\"prefill_intention\")\npng = os.path.join(PREFILL_RUN, f\"prefill_apology_layerwise_strength{PREFILL_STRENGTH}.png\")\n\nrun_module(\"introspect_repro.plotting.plot_prefill_intention\",\n           [\"--run-dir\", PREFILL_RUN, \"--strength\", PREFILL_STRENGTH, \"--save\", png])\n\nfrom IPython.display import Image, display\ndisplay(Image(filename=png))",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 7) Experiment D — **Intentional control** (think vs don’t think)\n\nProtocol (pp. 26–29; Appendix pp. 56–58): instruct **“Think about {word}”** versus **“Don’t think about {word}”**, record residuals during generation, and plot mean **cosine(model activations, concept vector)** vs layer for both cases.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "#@title Run intentional control across several layers\nLAYERS_TO_RUN = [0.50, 0.60, 0.66, 0.75, 0.85]  # as fractions of depth\nlayers = [max(0, min(NUM_LAYERS-1, int(round(fr*(NUM_LAYERS-1))))) for fr in LAYERS_TO_RUN]\n\nfor L in layers:\n    args = [\n        \"--model\", HF_MODEL,\n        \"--layer\", L,\n        \"--n-trials\", N_TRIALS_INT_CTRL\n    ]\n    if LOAD_IN_4BIT: args.append(\"--load-in-4bit\")\n    if LOAD_IN_8BIT: args.append(\"--load-in-8bit\")\n    if DTYPE and DTYPE != \"auto\": args += [\"--dtype\", DTYPE]\n\n    print(\"\\n=== Intentional control at layer\", L, \"===\")\n    run_module(\"introspect_repro.experiments.intentional_control\", args)",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "#@title Plot intentional control — mean cosine vs layer\nINTC_RUN = latest_run_dir(\"intentional_control\")\npng = os.path.join(INTC_RUN, \"intent_control_layerwise.png\")\n\nrun_module(\"introspect_repro.plotting.plot_intent_control\",\n           [\"--run-dir\", INTC_RUN, \"--save\", png])\n\nfrom IPython.display import Image, display\ndisplay(Image(filename=png))",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 8) (Optional) Summarize injected‑thoughts metrics as a table\n\nThe “awareness” column = **coherence ∧ affirmative ∧ correct identification**, matching the Appendix grader criteria (pp. 39–42).\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "#@title Build a summary table\nimport json, os, glob\nimport pandas as pd\n\nfrom introspect_repro.plotting.utils import load_results\n\nINJ = latest_run_dir(\"injected_thoughts\")\nrows = []\nfor (layer, strength, j, f) in load_results(INJ):\n    trials = j[\"trials\"]\n    coh = [t.get(\"coherence\") for t in trials]\n    aff = [t.get(\"affirmative\") for t in trials]\n    cor = [t.get(\"correct_identification\") for t in trials]\n    aware = [ (t.get(\"coherence\") and t.get(\"affirmative\") and t.get(\"correct_identification\")) for t in trials ]\n    rows.append(dict(layer=layer, strength=strength,\n                     n=len(trials),\n                     coherence=sum(1 for x in coh if x)/len(coh) if coh else 0.0,\n                     affirmative=sum(1 for x in aff if x)/len(aff) if aff else 0.0,\n                     correct_id=sum(1 for x in cor if x)/len(cor) if cor else 0.0,\n                     awareness=sum(1 for x in aware if x)/len(aware) if aware else 0.0))\ndf = pd.DataFrame(rows).sort_values([\"strength\",\"layer\"])\ndf.style.format({c:\"{:.3f}\" for c in [\"coherence\",\"affirmative\",\"correct_id\",\"awareness\"]})",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "---\n\n### Tips & troubleshooting\n\n- **VRAM errors** → switch to `LOAD_IN_4BIT=True`, set `DTYPE=\"bfloat16\"`, reduce `N_TRIALS_*`.  \n- **Judge time/cost** → use OpenAI `gpt-4o-mini` or Anthropic `haiku`-class as judges for quick passes.  \n- **Peaks by layer** → injected‑thoughts & thought‑vs‑text often peak near **~2/3 depth**; prefill sometimes peaks **earlier** (p. 24).  \n- **High strengths** → may cause incoherence/“brain damage” (pp. 13–14); stick to {1,2,4,8}.\n\n**Safety**: The Appendix includes a control prompt mentioning *donating to terrorist organizations*; the harness leaves that variant out by default.\n"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}