{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00314f81",
   "metadata": {},
   "source": [
    "# Reproducing *Emergent Introspective Awareness in LLMs* — End‑to‑End Notebook\n",
    "\n",
    "This notebook runs the four experiments from the paper and recreates the **layer‑wise** plots using the harness you downloaded.\n",
    "\n",
    "**What this notebook does**  \n",
    "1. Unpacks/sets up the harness (or uses an existing `src/` in this working directory).  \n",
    "2. Installs dependencies.  \n",
    "3. Lets you choose a model and runtime (4‑bit/8‑bit/FP16).  \n",
    "4. Runs:\n",
    "   - **Injected thoughts** (layer/strength sweep)\n",
    "   - **Thought vs text**\n",
    "   - **Prefill intention**\n",
    "   - **Intentional control**\n",
    "5. Produces **layer‑wise plots** that match the paper’s figures.\n",
    "\n",
    "**Sources from the paper** (Appendix) this notebook aligns to:\n",
    "- Concept vectors on the *final `:` token* in `Human: Tell me about {word}\\nAssistant:` and subtracting the mean baseline over 100 words (pp. **36–37**).  \n",
    "- Injection windows and experiment prompts (pp. **38–42**, **48–58**).  \n",
    "- Layer‑wise lines and typical peaks (pp. **15**, **21**, **24**, **28–29**).\n",
    "\n",
    "> Note: Exact **numbers** may differ (you’re using open‑source HF models; the paper used Anthropic internal models), but you should recover the **qualitative phenomena** and **layer/strength** trends.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ec3ff3",
   "metadata": {},
   "source": [
    "## 0) Get the code into this working directory\n",
    "\n",
    "- If you have the ZIP handy (e.g., `introspection_repro_with_plots.zip`), run the cell below to unpack it.  \n",
    "- If you already have a `src/` folder here, you can skip unpacking.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d53b2685",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found ./src — skipping unzip.\n"
     ]
    }
   ],
   "source": [
    "#@title Unpack the harness zip (edit ZIP_PATH if needed)\n",
    "import os, zipfile, glob\n",
    "\n",
    "# Change this to the name/path of the ZIP you uploaded to this notebook runtime\n",
    "ZIP_PATH = 'introspection_repro_with_plots.zip'  # or 'introspection_repro.zip'\n",
    "\n",
    "if os.path.isdir('src'):\n",
    "    print('Found ./src — skipping unzip.')\n",
    "elif os.path.exists(ZIP_PATH):\n",
    "    with zipfile.ZipFile(ZIP_PATH, 'r') as z:\n",
    "        z.extractall('.')\n",
    "    print('Unzipped:', ZIP_PATH)\n",
    "    print('Top-level entries:', os.listdir('.')[:10])\n",
    "else:\n",
    "    print('No ./src and no ZIP_PATH found. Please upload the ZIP to this directory or clone the repo here.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f965f1",
   "metadata": {},
   "source": [
    "## 1) Install dependencies\n",
    "\n",
    "- Make sure your CUDA/Torch versions match your GPU drivers.\n",
    "- If you already have Torch installed, you can skip reinstalling it.\n",
    "\n",
    "> **Tip:** For CUDA 12.1+ wheels on Linux: `pip install torch --index-url https://download.pytorch.org/whl/cu121`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a940f3ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers>=4.44.0 in ./.venv/lib/python3.13/site-packages (from -r requirements.txt (line 1)) (4.57.1)\n",
      "Requirement already satisfied: accelerate>=0.33.0 in ./.venv/lib/python3.13/site-packages (from -r requirements.txt (line 2)) (1.11.0)\n",
      "Requirement already satisfied: torch>=2.2.0 in ./.venv/lib/python3.13/site-packages (from -r requirements.txt (line 3)) (2.9.0)\n",
      "Requirement already satisfied: bitsandbytes>=0.43.0 in ./.venv/lib/python3.13/site-packages (from -r requirements.txt (line 4)) (0.48.2)\n",
      "Requirement already satisfied: einops>=0.7.0 in ./.venv/lib/python3.13/site-packages (from -r requirements.txt (line 5)) (0.8.1)\n",
      "Requirement already satisfied: numpy>=1.26.0 in ./.venv/lib/python3.13/site-packages (from -r requirements.txt (line 6)) (2.3.4)\n",
      "Requirement already satisfied: pandas>=2.2.0 in ./.venv/lib/python3.13/site-packages (from -r requirements.txt (line 7)) (2.3.3)\n",
      "Requirement already satisfied: tqdm>=4.66.0 in ./.venv/lib/python3.13/site-packages (from -r requirements.txt (line 8)) (4.67.1)\n",
      "Requirement already satisfied: matplotlib>=3.8.0 in ./.venv/lib/python3.13/site-packages (from -r requirements.txt (line 9)) (3.10.7)\n",
      "Requirement already satisfied: anthropic>=0.32.0 in ./.venv/lib/python3.13/site-packages (from -r requirements.txt (line 10)) (0.72.0)\n",
      "Requirement already satisfied: openai>=1.45.0 in ./.venv/lib/python3.13/site-packages (from -r requirements.txt (line 11)) (2.6.1)\n",
      "Requirement already satisfied: httpx>=0.27.0 in ./.venv/lib/python3.13/site-packages (from -r requirements.txt (line 12)) (0.28.1)\n",
      "Requirement already satisfied: python-dotenv>=1.0.1 in ./.venv/lib/python3.13/site-packages (from -r requirements.txt (line 13)) (1.2.1)\n",
      "Requirement already satisfied: tiktoken>=0.12.0 in ./.venv/lib/python3.13/site-packages (from -r requirements.txt (line 14)) (0.12.0)\n",
      "Requirement already satisfied: protobuf>=6.33.0 in ./.venv/lib/python3.13/site-packages (from -r requirements.txt (line 15)) (6.33.0)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.13/site-packages (from transformers>=4.44.0->-r requirements.txt (line 1)) (3.20.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in ./.venv/lib/python3.13/site-packages (from transformers>=4.44.0->-r requirements.txt (line 1)) (0.36.0)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.13/site-packages (from transformers>=4.44.0->-r requirements.txt (line 1)) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./.venv/lib/python3.13/site-packages (from transformers>=4.44.0->-r requirements.txt (line 1)) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./.venv/lib/python3.13/site-packages (from transformers>=4.44.0->-r requirements.txt (line 1)) (2025.10.23)\n",
      "Requirement already satisfied: requests in ./.venv/lib/python3.13/site-packages (from transformers>=4.44.0->-r requirements.txt (line 1)) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in ./.venv/lib/python3.13/site-packages (from transformers>=4.44.0->-r requirements.txt (line 1)) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in ./.venv/lib/python3.13/site-packages (from transformers>=4.44.0->-r requirements.txt (line 1)) (0.6.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./.venv/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers>=4.44.0->-r requirements.txt (line 1)) (2025.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.venv/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers>=4.44.0->-r requirements.txt (line 1)) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in ./.venv/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers>=4.44.0->-r requirements.txt (line 1)) (1.2.0)\n",
      "Requirement already satisfied: psutil in ./.venv/lib/python3.13/site-packages (from accelerate>=0.33.0->-r requirements.txt (line 2)) (7.1.3)\n",
      "Requirement already satisfied: setuptools in ./.venv/lib/python3.13/site-packages (from torch>=2.2.0->-r requirements.txt (line 3)) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in ./.venv/lib/python3.13/site-packages (from torch>=2.2.0->-r requirements.txt (line 3)) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in ./.venv/lib/python3.13/site-packages (from torch>=2.2.0->-r requirements.txt (line 3)) (3.5)\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.13/site-packages (from torch>=2.2.0->-r requirements.txt (line 3)) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in ./.venv/lib/python3.13/site-packages (from torch>=2.2.0->-r requirements.txt (line 3)) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in ./.venv/lib/python3.13/site-packages (from torch>=2.2.0->-r requirements.txt (line 3)) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in ./.venv/lib/python3.13/site-packages (from torch>=2.2.0->-r requirements.txt (line 3)) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in ./.venv/lib/python3.13/site-packages (from torch>=2.2.0->-r requirements.txt (line 3)) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in ./.venv/lib/python3.13/site-packages (from torch>=2.2.0->-r requirements.txt (line 3)) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in ./.venv/lib/python3.13/site-packages (from torch>=2.2.0->-r requirements.txt (line 3)) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in ./.venv/lib/python3.13/site-packages (from torch>=2.2.0->-r requirements.txt (line 3)) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in ./.venv/lib/python3.13/site-packages (from torch>=2.2.0->-r requirements.txt (line 3)) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in ./.venv/lib/python3.13/site-packages (from torch>=2.2.0->-r requirements.txt (line 3)) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in ./.venv/lib/python3.13/site-packages (from torch>=2.2.0->-r requirements.txt (line 3)) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in ./.venv/lib/python3.13/site-packages (from torch>=2.2.0->-r requirements.txt (line 3)) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in ./.venv/lib/python3.13/site-packages (from torch>=2.2.0->-r requirements.txt (line 3)) (3.3.20)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in ./.venv/lib/python3.13/site-packages (from torch>=2.2.0->-r requirements.txt (line 3)) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in ./.venv/lib/python3.13/site-packages (from torch>=2.2.0->-r requirements.txt (line 3)) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in ./.venv/lib/python3.13/site-packages (from torch>=2.2.0->-r requirements.txt (line 3)) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.5.0 in ./.venv/lib/python3.13/site-packages (from torch>=2.2.0->-r requirements.txt (line 3)) (3.5.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.13/site-packages (from pandas>=2.2.0->-r requirements.txt (line 7)) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.13/site-packages (from pandas>=2.2.0->-r requirements.txt (line 7)) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.venv/lib/python3.13/site-packages (from pandas>=2.2.0->-r requirements.txt (line 7)) (2025.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in ./.venv/lib/python3.13/site-packages (from matplotlib>=3.8.0->-r requirements.txt (line 9)) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in ./.venv/lib/python3.13/site-packages (from matplotlib>=3.8.0->-r requirements.txt (line 9)) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in ./.venv/lib/python3.13/site-packages (from matplotlib>=3.8.0->-r requirements.txt (line 9)) (4.60.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in ./.venv/lib/python3.13/site-packages (from matplotlib>=3.8.0->-r requirements.txt (line 9)) (1.4.9)\n",
      "Requirement already satisfied: pillow>=8 in ./.venv/lib/python3.13/site-packages (from matplotlib>=3.8.0->-r requirements.txt (line 9)) (12.0.0)\n",
      "Requirement already satisfied: pyparsing>=3 in ./.venv/lib/python3.13/site-packages (from matplotlib>=3.8.0->-r requirements.txt (line 9)) (3.2.5)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in ./.venv/lib/python3.13/site-packages (from anthropic>=0.32.0->-r requirements.txt (line 10)) (4.11.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in ./.venv/lib/python3.13/site-packages (from anthropic>=0.32.0->-r requirements.txt (line 10)) (1.9.0)\n",
      "Requirement already satisfied: docstring-parser<1,>=0.15 in ./.venv/lib/python3.13/site-packages (from anthropic>=0.32.0->-r requirements.txt (line 10)) (0.17.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in ./.venv/lib/python3.13/site-packages (from anthropic>=0.32.0->-r requirements.txt (line 10)) (0.11.1)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in ./.venv/lib/python3.13/site-packages (from anthropic>=0.32.0->-r requirements.txt (line 10)) (2.12.3)\n",
      "Requirement already satisfied: sniffio in ./.venv/lib/python3.13/site-packages (from anthropic>=0.32.0->-r requirements.txt (line 10)) (1.3.1)\n",
      "Requirement already satisfied: certifi in ./.venv/lib/python3.13/site-packages (from httpx>=0.27.0->-r requirements.txt (line 12)) (2025.10.5)\n",
      "Requirement already satisfied: httpcore==1.* in ./.venv/lib/python3.13/site-packages (from httpx>=0.27.0->-r requirements.txt (line 12)) (1.0.9)\n",
      "Requirement already satisfied: idna in ./.venv/lib/python3.13/site-packages (from httpx>=0.27.0->-r requirements.txt (line 12)) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in ./.venv/lib/python3.13/site-packages (from httpcore==1.*->httpx>=0.27.0->-r requirements.txt (line 12)) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in ./.venv/lib/python3.13/site-packages (from pydantic<3,>=1.9.0->anthropic>=0.32.0->-r requirements.txt (line 10)) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.4 in ./.venv/lib/python3.13/site-packages (from pydantic<3,>=1.9.0->anthropic>=0.32.0->-r requirements.txt (line 10)) (2.41.4)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in ./.venv/lib/python3.13/site-packages (from pydantic<3,>=1.9.0->anthropic>=0.32.0->-r requirements.txt (line 10)) (0.4.2)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.13/site-packages (from python-dateutil>=2.8.2->pandas>=2.2.0->-r requirements.txt (line 7)) (1.17.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./.venv/lib/python3.13/site-packages (from requests->transformers>=4.44.0->-r requirements.txt (line 1)) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.13/site-packages (from requests->transformers>=4.44.0->-r requirements.txt (line 1)) (2.5.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.venv/lib/python3.13/site-packages (from sympy>=1.13.3->torch>=2.2.0->-r requirements.txt (line 3)) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.13/site-packages (from jinja2->torch>=2.2.0->-r requirements.txt (line 3)) (3.0.3)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/home/keithvertrees/Projects/ai-introspection/.venv/bin/python -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#@title Install Python packages (uncomment Torch line if needed)\n",
    "# !pip install --upgrade pip\n",
    "# If you need a matching Torch build, uncomment ONE of these lines:\n",
    "# !pip install torch --index-url https://download.pytorch.org/whl/cu121   # CUDA 12.1\n",
    "# !pip install torch --index-url https://download.pytorch.org/whl/cu118   # CUDA 11.8\n",
    "\n",
    "# Install the experiment harness deps\n",
    "!.venv/bin/pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f705f819",
   "metadata": {},
   "source": [
    "## 2) Wire up API keys for the **LLM grader**\n",
    "\n",
    "The experiments use an **LLM judge** (Anthropic / OpenAI / OpenRouter) with the paper’s grading prompts (Appendix pp. 39–42, 54).  \n",
    "Set any one (or more) of these in the environment for this notebook process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a811a702",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: project .venv directory not found. Create it via `python -m venv .venv`.\n",
      "Loaded variables from .env (existing environment values are preserved).\n",
      "Anthropic: set\n",
      "OpenAI   : set\n",
      "OpenRouter: set\n",
      "HuggingFace: not set\n"
     ]
    }
   ],
   "source": [
    "#@title Load environment variables from the .env file\n",
    "from pathlib import Path\n",
    "import importlib\n",
    "import os\n",
    "import sys\n",
    "\n",
    "\n",
    "def _load_package():\n",
    "    try:\n",
    "        return importlib.import_module(\"introspect_repro\")\n",
    "    except ModuleNotFoundError:\n",
    "        search_roots = [Path.cwd().resolve()]\n",
    "        search_roots += list(search_roots[0].parents)\n",
    "        for root in search_roots:\n",
    "            src_dir = root / \"src\"\n",
    "            if not src_dir.is_dir():\n",
    "                continue\n",
    "            if str(src_dir) not in sys.path:\n",
    "                sys.path.append(str(src_dir))\n",
    "            try:\n",
    "                return importlib.import_module(\"introspect_repro\")\n",
    "            except ModuleNotFoundError:\n",
    "                continue\n",
    "        raise\n",
    "\n",
    "\n",
    "pkg = _load_package()\n",
    "if not hasattr(pkg, \"activate_local_venv\"):\n",
    "    pkg = importlib.reload(pkg)\n",
    "\n",
    "activate_local_venv = getattr(pkg, \"activate_local_venv\")\n",
    "load_project_env = getattr(pkg, \"load_project_env\")\n",
    "get_hf_token = getattr(pkg, \"get_hf_token\", lambda: None)\n",
    "hf_token_env_keys = getattr(pkg, \"HF_TOKEN_ENV_KEYS\", (\n",
    "    \"HUGGINGFACEHUB_API_TOKEN\",\n",
    "    \"HUGGINGFACE_TOKEN\",\n",
    "    \"HF_TOKEN\",\n",
    "    \"HF_API_TOKEN\",\n",
    "))\n",
    "\n",
    "activate_local_venv()\n",
    "load_project_env()\n",
    "\n",
    "project_root = Path(pkg.__file__).resolve().parent.parent\n",
    "project_venv = project_root / \".venv\"\n",
    "interpreter_path = Path(sys.executable)\n",
    "if project_venv.exists():\n",
    "    if project_venv in interpreter_path.parents:\n",
    "        print(f\"Using interpreter: {interpreter_path}\")\n",
    "    else:\n",
    "        print(f\"Warning: kernel interpreter {interpreter_path} is outside .venv; added .venv site-packages to sys.path.\")\n",
    "else:\n",
    "    print(\"Warning: project .venv directory not found. Create it via `python -m venv .venv`.\")\n",
    "\n",
    "print(\"Loaded variables from .env (existing environment values are preserved).\")\n",
    "status_labels = (\n",
    "    (\"ANTHROPIC_API_KEY\", \"Anthropic\"),\n",
    "    (\"OPENAI_API_KEY\", \"OpenAI   \"),\n",
    "    (\"OPENROUTER_API_KEY\", \"OpenRouter\"),\n",
    ")\n",
    "for key, label in status_labels:\n",
    "    print(f\"{label}: {'set' if os.environ.get(key) else 'not set'}\")\n",
    "\n",
    "hf_token = get_hf_token()\n",
    "if hf_token:\n",
    "    first_key = next((key for key in hf_token_env_keys if os.environ.get(key)), None)\n",
    "    if first_key:\n",
    "        print(f\"HuggingFace: set ({first_key})\")\n",
    "    else:\n",
    "        print(\"HuggingFace: set\")\n",
    "else:\n",
    "    print(\"HuggingFace: not set\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "934cbe06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anthropic: set\n",
      "OpenAI   : set\n",
      "OpenRouter: set\n"
     ]
    }
   ],
   "source": [
    "#@title Set API keys for grading (fill values or leave to inherit from your environment)\n",
    "import os\n",
    "\n",
    "# Fill in only the ones you plan to use. Leaving blank keeps the existing environment value.\n",
    "ANTHROPIC = \"\"  # e.g., \"sk-ant-...\"\n",
    "OPENAI = \"\"     # e.g., \"sk-...\"\n",
    "OPENROUTER = \"\" # e.g., \"or-...\"\n",
    "\n",
    "if ANTHROPIC: os.environ['ANTHROPIC_API_KEY'] = ANTHROPIC\n",
    "if OPENAI:    os.environ['OPENAI_API_KEY'] = OPENAI\n",
    "if OPENROUTER:os.environ['OPENROUTER_API_KEY'] = OPENROUTER\n",
    "\n",
    "print(\"Anthropic:\", \"set\" if os.environ.get(\"ANTHROPIC_API_KEY\") else \"not set\")\n",
    "print(\"OpenAI   :\", \"set\" if os.environ.get(\"OPENAI_API_KEY\") else \"not set\")\n",
    "print(\"OpenRouter:\", \"set\" if os.environ.get(\"OPENROUTER_API_KEY\") else \"not set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de38bde",
   "metadata": {},
   "source": [
    "## 3) Configure model and runtime\n",
    "\n",
    "- **HF model**: any decoder‑only chat/instruct model with access to internals (e.g., Llama‑3‑8B‑Instruct, Qwen‑2.5‑7B/14B, Mixtral).  \n",
    "- **Precision**: FP16 if you have VRAM; otherwise 8‑bit/4‑bit.  \n",
    "- **Judge**: pick provider/model for grading.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83fa7c90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: meta-llama/Meta-Llama-3-8B\n",
      "Judge: openai / gpt-5-mini\n"
     ]
    }
   ],
   "source": [
    "#@title Choose your HF model and judge\n",
    "HF_MODEL = \"meta-llama/Meta-Llama-3-8B\"  #@param {type:\"string\"}\n",
    "LOAD_IN_4BIT = True                                #@param {type:\"boolean\"}\n",
    "LOAD_IN_8BIT = False                               #@param {type:\"boolean\"}\n",
    "DTYPE = \"bfloat16\"                                  #@param [\"bfloat16\",\"float16\",\"auto\"]\n",
    "\n",
    "JUDGE_PROVIDER = \"openai\"                           #@param [\"openai\",\"anthropic\",\"openrouter\"]\n",
    "JUDGE_MODEL = \"gpt-5-mini\"                         #@param {type:\"string\"}\n",
    "\n",
    "N_TRIALS_INJECTED = 30                              #@param {type:\"integer\"}\n",
    "N_TRIALS_TVT = 50                                   #@param {type:\"integer\"}\n",
    "N_TRIALS_PREFILL = 50                               #@param {type:\"integer\"}\n",
    "N_TRIALS_INT_CTRL = 16                              #@param {type:\"integer\"}\n",
    "\n",
    "print(\"Model:\", HF_MODEL)\n",
    "print(\"Judge:\", JUDGE_PROVIDER, \"/\", JUDGE_MODEL)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd64e83",
   "metadata": {},
   "source": [
    "### Add the harness to `sys.path` and compute a good default layer\n",
    "\n",
    "We avoid loading full weights just to count layers by reading the config.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "612e1436",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in ./.venv/lib/python3.13/site-packages (4.57.1)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.13/site-packages (from transformers) (3.20.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in ./.venv/lib/python3.13/site-packages (from transformers) (0.36.0)\n",
      "Requirement already satisfied: numpy>=1.17 in ./.venv/lib/python3.13/site-packages (from transformers) (2.3.4)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.13/site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./.venv/lib/python3.13/site-packages (from transformers) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./.venv/lib/python3.13/site-packages (from transformers) (2025.10.23)\n",
      "Requirement already satisfied: requests in ./.venv/lib/python3.13/site-packages (from transformers) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in ./.venv/lib/python3.13/site-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in ./.venv/lib/python3.13/site-packages (from transformers) (0.6.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in ./.venv/lib/python3.13/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./.venv/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.venv/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in ./.venv/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./.venv/lib/python3.13/site-packages (from requests->transformers) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.13/site-packages (from requests->transformers) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.13/site-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.13/site-packages (from requests->transformers) (2025.10.5)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/home/keithvertrees/Projects/ai-introspection/.venv/bin/python -m pip install --upgrade pip\u001b[0m\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'transformers'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m     10\u001b[39m         sys.path.insert(\u001b[32m0\u001b[39m, os.path.abspath(c))\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# Light‑weight: use AutoConfig to count layers\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AutoConfig\n\u001b[32m     14\u001b[39m cfg = AutoConfig.from_pretrained(HF_MODEL)\n\u001b[32m     15\u001b[39m NUM_LAYERS = \u001b[38;5;28mgetattr\u001b[39m(cfg, \u001b[33m\"\u001b[39m\u001b[33mnum_hidden_layers\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(cfg, \u001b[33m\"\u001b[39m\u001b[33mn_layer\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'transformers'"
     ]
    }
   ],
   "source": [
    "#@title Prepare imports and compute a 2/3‑depth layer guess\n",
    "!.venv/bin/pip install transformers\n",
    "import os, sys, glob, math, subprocess, shlex, json, pathlib, time\n",
    "from IPython.display import display, Image\n",
    "\n",
    "# Add ./src to import path\n",
    "SRC_CANDIDATES = [\"./src\", \"../src\", \"/workspace/src\"]\n",
    "for c in SRC_CANDIDATES:\n",
    "    if os.path.isdir(c) and os.path.abspath(c) not in sys.path:\n",
    "        sys.path.insert(0, os.path.abspath(c))\n",
    "\n",
    "# Light‑weight: use AutoConfig to count layers\n",
    "from transformers import AutoConfig\n",
    "cfg = AutoConfig.from_pretrained(HF_MODEL)\n",
    "NUM_LAYERS = getattr(cfg, \"num_hidden_layers\", None) or getattr(cfg, \"n_layer\", None)\n",
    "LAYER_2_3 = int(round(0.66 * (NUM_LAYERS - 1))) if NUM_LAYERS else None\n",
    "print(\"num_hidden_layers:\", NUM_LAYERS, \"|  ~2/3 layer:\", LAYER_2_3)\n",
    "\n",
    "# Convenience to run a python module from this kernel\n",
    "def run_module(modname, args_list):\n",
    "    cmd = [sys.executable, \"-m\", modname] + list(map(str, args_list))\n",
    "    print(\">>>\", \" \".join(cmd))\n",
    "    subprocess.run(cmd, check=True)\n",
    "\n",
    "def latest_run_dir(exp_name):\n",
    "    cands = glob.glob(os.path.join(\"runs\", \"*\", exp_name))\n",
    "    return max(cands, key=os.path.getmtime) if cands else None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e7e7212",
   "metadata": {},
   "source": [
    "## 4) Experiment A — **Injected thoughts** (layer & strength sweeps)\n",
    "\n",
    "Protocol (Appendix pp. 36–42): compute concept vectors on the `Assistant:` colon token, subtract baseline mean (100 words), then inject on the Assistant turn of the **“injected thoughts”** prompt and judge **coherence ∧ affirmative ∧ correct identification (before saying the word)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2ac2d365",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> /home/keithvertrees/envs/torch-gpu/bin/python -m src.introspect_repro.experiments.injected_thoughts --model MaziyarPanahi/Meta-Llama-3-8B-Instruct-GGUF --judge-provider openai --judge-model gpt-5-mini --n-trials 30 --sweep-layers 0.6 0.66 0.7 0.75 0.8 --strengths 1 2 4 --load-in-4bit --dtype bfloat16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.\n",
      "Traceback (most recent call last):\n",
      "  File \u001b[35m\"/home/keithvertrees/Projects/ai-introspection/.venv/lib/python3.13/site-packages/transformers/convert_slow_tokenizer.py\"\u001b[0m, line \u001b[35m1783\u001b[0m, in \u001b[35mconvert_slow_tokenizer\u001b[0m\n",
      "    ).\u001b[31mconverted\u001b[0m\u001b[1;31m()\u001b[0m\n",
      "      \u001b[31m~~~~~~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "  File \u001b[35m\"/home/keithvertrees/Projects/ai-introspection/.venv/lib/python3.13/site-packages/transformers/convert_slow_tokenizer.py\"\u001b[0m, line \u001b[35m1677\u001b[0m, in \u001b[35mconverted\u001b[0m\n",
      "    tokenizer = self.tokenizer()\n",
      "  File \u001b[35m\"/home/keithvertrees/Projects/ai-introspection/.venv/lib/python3.13/site-packages/transformers/convert_slow_tokenizer.py\"\u001b[0m, line \u001b[35m1670\u001b[0m, in \u001b[35mtokenizer\u001b[0m\n",
      "    vocab_scores, merges = \u001b[31mself.extract_vocab_merges_from_model\u001b[0m\u001b[1;31m(self.vocab_file)\u001b[0m\n",
      "                           \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/home/keithvertrees/Projects/ai-introspection/.venv/lib/python3.13/site-packages/transformers/convert_slow_tokenizer.py\"\u001b[0m, line \u001b[35m1646\u001b[0m, in \u001b[35mextract_vocab_merges_from_model\u001b[0m\n",
      "    bpe_ranks = load_tiktoken_bpe(tiktoken_url)\n",
      "  File \u001b[35m\"/home/keithvertrees/Projects/ai-introspection/.venv/lib/python3.13/site-packages/tiktoken/load.py\"\u001b[0m, line \u001b[35m162\u001b[0m, in \u001b[35mload_tiktoken_bpe\u001b[0m\n",
      "    contents = read_file_cached(tiktoken_bpe_file, expected_hash)\n",
      "  File \u001b[35m\"/home/keithvertrees/Projects/ai-introspection/.venv/lib/python3.13/site-packages/tiktoken/load.py\"\u001b[0m, line \u001b[35m52\u001b[0m, in \u001b[35mread_file_cached\u001b[0m\n",
      "    cache_key = hashlib.sha1(\u001b[1;31mblobpath.encode\u001b[0m()).hexdigest()\n",
      "                             \u001b[1;31m^^^^^^^^^^^^^^^\u001b[0m\n",
      "\u001b[1;35mAttributeError\u001b[0m: \u001b[35m'NoneType' object has no attribute 'encode'\u001b[0m\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \u001b[35m\"<frozen runpy>\"\u001b[0m, line \u001b[35m198\u001b[0m, in \u001b[35m_run_module_as_main\u001b[0m\n",
      "  File \u001b[35m\"<frozen runpy>\"\u001b[0m, line \u001b[35m88\u001b[0m, in \u001b[35m_run_code\u001b[0m\n",
      "  File \u001b[35m\"/home/keithvertrees/Projects/ai-introspection/src/introspect_repro/experiments/injected_thoughts.py\"\u001b[0m, line \u001b[35m69\u001b[0m, in \u001b[35m<module>\u001b[0m\n",
      "    \u001b[31mmain\u001b[0m\u001b[1;31m()\u001b[0m\n",
      "    \u001b[31m~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "  File \u001b[35m\"/home/keithvertrees/Projects/ai-introspection/src/introspect_repro/experiments/injected_thoughts.py\"\u001b[0m, line \u001b[35m29\u001b[0m, in \u001b[35mmain\u001b[0m\n",
      "    model, tok = \u001b[31mload_model_and_tokenizer\u001b[0m\u001b[1;31m(args.model, device=args.device,\u001b[0m\n",
      "                 \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "                                          \u001b[1;31mload_in_4bit=args.load_in_4bit, load_in_8bit=args.load_in_8bit,\u001b[0m\n",
      "                                          \u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "                                          \u001b[1;31mdtype=args.dtype)\u001b[0m\n",
      "                                          \u001b[1;31m^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/home/keithvertrees/Projects/ai-introspection/src/introspect_repro/models.py\"\u001b[0m, line \u001b[35m29\u001b[0m, in \u001b[35mload_model_and_tokenizer\u001b[0m\n",
      "    tok = AutoTokenizer.from_pretrained(model_name, use_fast=True, **token_kwargs)\n",
      "  File \u001b[35m\"/home/keithvertrees/Projects/ai-introspection/.venv/lib/python3.13/site-packages/transformers/models/auto/tokenization_auto.py\"\u001b[0m, line \u001b[35m1159\u001b[0m, in \u001b[35mfrom_pretrained\u001b[0m\n",
      "    return \u001b[31mtokenizer_class_fast.from_pretrained\u001b[0m\u001b[1;31m(pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n",
      "           \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/home/keithvertrees/Projects/ai-introspection/.venv/lib/python3.13/site-packages/transformers/tokenization_utils_base.py\"\u001b[0m, line \u001b[35m2097\u001b[0m, in \u001b[35mfrom_pretrained\u001b[0m\n",
      "    return \u001b[31mcls._from_pretrained\u001b[0m\u001b[1;31m(\u001b[0m\n",
      "           \u001b[31m~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^\u001b[0m\n",
      "        \u001b[1;31mresolved_vocab_files,\u001b[0m\n",
      "        \u001b[1;31m^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "    ...<9 lines>...\n",
      "        \u001b[1;31m**kwargs,\u001b[0m\n",
      "        \u001b[1;31m^^^^^^^^^\u001b[0m\n",
      "    \u001b[1;31m)\u001b[0m\n",
      "    \u001b[1;31m^\u001b[0m\n",
      "  File \u001b[35m\"/home/keithvertrees/Projects/ai-introspection/.venv/lib/python3.13/site-packages/transformers/tokenization_utils_base.py\"\u001b[0m, line \u001b[35m2343\u001b[0m, in \u001b[35m_from_pretrained\u001b[0m\n",
      "    tokenizer = cls(*init_inputs, **init_kwargs)\n",
      "  File \u001b[35m\"/home/keithvertrees/Projects/ai-introspection/.venv/lib/python3.13/site-packages/transformers/models/llama/tokenization_llama_fast.py\"\u001b[0m, line \u001b[35m154\u001b[0m, in \u001b[35m__init__\u001b[0m\n",
      "    \u001b[31msuper().__init__\u001b[0m\u001b[1;31m(\u001b[0m\n",
      "    \u001b[31m~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^\u001b[0m\n",
      "        \u001b[1;31mvocab_file=vocab_file,\u001b[0m\n",
      "        \u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "    ...<10 lines>...\n",
      "        \u001b[1;31m**kwargs,\u001b[0m\n",
      "        \u001b[1;31m^^^^^^^^^\u001b[0m\n",
      "    \u001b[1;31m)\u001b[0m\n",
      "    \u001b[1;31m^\u001b[0m\n",
      "  File \u001b[35m\"/home/keithvertrees/Projects/ai-introspection/.venv/lib/python3.13/site-packages/transformers/tokenization_utils_fast.py\"\u001b[0m, line \u001b[35m139\u001b[0m, in \u001b[35m__init__\u001b[0m\n",
      "    fast_tokenizer = convert_slow_tokenizer(self, from_tiktoken=True)\n",
      "  File \u001b[35m\"/home/keithvertrees/Projects/ai-introspection/.venv/lib/python3.13/site-packages/transformers/convert_slow_tokenizer.py\"\u001b[0m, line \u001b[35m1785\u001b[0m, in \u001b[35mconvert_slow_tokenizer\u001b[0m\n",
      "    raise ValueError(\n",
      "    ...<3 lines>...\n",
      "    )\n",
      "\u001b[1;35mValueError\u001b[0m: \u001b[35mConverting from SentencePiece and Tiktoken failed, if a converter for SentencePiece is available, provide a model path with a SentencePiece tokenizer.model file.Currently available slow->fast converters: ['AlbertTokenizer', 'BartTokenizer', 'BarthezTokenizer', 'BertTokenizer', 'BigBirdTokenizer', 'BlenderbotTokenizer', 'CamembertTokenizer', 'CLIPTokenizer', 'CodeGenTokenizer', 'ConvBertTokenizer', 'DebertaTokenizer', 'DebertaV2Tokenizer', 'DistilBertTokenizer', 'DPRReaderTokenizer', 'DPRQuestionEncoderTokenizer', 'DPRContextEncoderTokenizer', 'ElectraTokenizer', 'FNetTokenizer', 'FunnelTokenizer', 'GPT2Tokenizer', 'HerbertTokenizer', 'LayoutLMTokenizer', 'LayoutLMv2Tokenizer', 'LayoutLMv3Tokenizer', 'LayoutXLMTokenizer', 'LongformerTokenizer', 'LEDTokenizer', 'LxmertTokenizer', 'MarkupLMTokenizer', 'MBartTokenizer', 'MBart50Tokenizer', 'MPNetTokenizer', 'MobileBertTokenizer', 'MvpTokenizer', 'NllbTokenizer', 'OpenAIGPTTokenizer', 'PegasusTokenizer', 'Qwen2Tokenizer', 'RealmTokenizer', 'ReformerTokenizer', 'RemBertTokenizer', 'RetriBertTokenizer', 'RobertaTokenizer', 'RoFormerTokenizer', 'SeamlessM4TTokenizer', 'SqueezeBertTokenizer', 'T5Tokenizer', 'UdopTokenizer', 'WhisperTokenizer', 'XLMRobertaTokenizer', 'XLNetTokenizer', 'SplinterTokenizer', 'XGLMTokenizer', 'LlamaTokenizer', 'CodeLlamaTokenizer', 'GemmaTokenizer', 'Phi3Tokenizer']\u001b[0m\n"
     ]
    },
    {
     "ename": "CalledProcessError",
     "evalue": "Command '['/home/keithvertrees/envs/torch-gpu/bin/python', '-m', 'src.introspect_repro.experiments.injected_thoughts', '--model', 'MaziyarPanahi/Meta-Llama-3-8B-Instruct-GGUF', '--judge-provider', 'openai', '--judge-model', 'gpt-5-mini', '--n-trials', '30', '--sweep-layers', '0.6', '0.66', '0.7', '0.75', '0.8', '--strengths', '1', '2', '4', '--load-in-4bit', '--dtype', 'bfloat16']' returned non-zero exit status 1.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mCalledProcessError\u001b[39m                        Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m LOAD_IN_8BIT: args.append(\u001b[33m\"\u001b[39m\u001b[33m--load-in-8bit\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m DTYPE \u001b[38;5;129;01mand\u001b[39;00m DTYPE != \u001b[33m\"\u001b[39m\u001b[33mauto\u001b[39m\u001b[33m\"\u001b[39m: args += [\u001b[33m\"\u001b[39m\u001b[33m--dtype\u001b[39m\u001b[33m\"\u001b[39m, DTYPE]\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m \u001b[43mrun_module\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msrc.introspect_repro.experiments.injected_thoughts\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 22\u001b[39m, in \u001b[36mrun_module\u001b[39m\u001b[34m(modname, args_list)\u001b[39m\n\u001b[32m     20\u001b[39m cmd = [sys.executable, \u001b[33m\"\u001b[39m\u001b[33m-m\u001b[39m\u001b[33m\"\u001b[39m, modname] + \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28mstr\u001b[39m, args_list))\n\u001b[32m     21\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m>>>\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m.join(cmd))\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m \u001b[43msubprocess\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcmd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/mise/installs/python/3.13.5/lib/python3.13/subprocess.py:577\u001b[39m, in \u001b[36mrun\u001b[39m\u001b[34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[39m\n\u001b[32m    575\u001b[39m     retcode = process.poll()\n\u001b[32m    576\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m check \u001b[38;5;129;01mand\u001b[39;00m retcode:\n\u001b[32m--> \u001b[39m\u001b[32m577\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m CalledProcessError(retcode, process.args,\n\u001b[32m    578\u001b[39m                                  output=stdout, stderr=stderr)\n\u001b[32m    579\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m CompletedProcess(process.args, retcode, stdout, stderr)\n",
      "\u001b[31mCalledProcessError\u001b[39m: Command '['/home/keithvertrees/envs/torch-gpu/bin/python', '-m', 'src.introspect_repro.experiments.injected_thoughts', '--model', 'MaziyarPanahi/Meta-Llama-3-8B-Instruct-GGUF', '--judge-provider', 'openai', '--judge-model', 'gpt-5-mini', '--n-trials', '30', '--sweep-layers', '0.6', '0.66', '0.7', '0.75', '0.8', '--strengths', '1', '2', '4', '--load-in-4bit', '--dtype', 'bfloat16']' returned non-zero exit status 1."
     ]
    }
   ],
   "source": [
    "#@title Run injected thoughts (layer/strength sweep)\n",
    "LAYER_FRACTIONS = [0.60, 0.66, 0.70, 0.75, 0.80]  #@param\n",
    "STRENGTHS = [1,2,4]                               #@param\n",
    "\n",
    "args = [\n",
    "    \"--model\", HF_MODEL,\n",
    "    \"--judge-provider\", JUDGE_PROVIDER,\n",
    "    \"--judge-model\", JUDGE_MODEL,\n",
    "    \"--n-trials\", N_TRIALS_INJECTED,\n",
    "    \"--sweep-layers\", *LAYER_FRACTIONS,\n",
    "    \"--strengths\", *STRENGTHS\n",
    "]\n",
    "if LOAD_IN_4BIT: args.append(\"--load-in-4bit\")\n",
    "if LOAD_IN_8BIT: args.append(\"--load-in-8bit\")\n",
    "if DTYPE and DTYPE != \"auto\": args += [\"--dtype\", DTYPE]\n",
    "\n",
    "run_module(\"src.introspect_repro.experiments.injected_thoughts\", args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9904dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Plot injected thoughts: layer‑wise lines\n",
    "INJECTED_RUN = latest_run_dir(\"injected_thoughts\")\n",
    "print(\"Run dir:\", INJECTED_RUN)\n",
    "\n",
    "# Pick one strength to match the paper’s typical layer‑wise figure (strength≈2)\n",
    "PLOT_STRENGTH = 2\n",
    "png = os.path.join(INJECTED_RUN, f\"layerwise_strength{PLOT_STRENGTH}.png\")\n",
    "\n",
    "run_module(\"introspect_repro.plotting.plot_injected_thoughts\",\n",
    "           [\"--run-dir\", INJECTED_RUN, \"--strength\", PLOT_STRENGTH, \"--save\", png])\n",
    "\n",
    "display(Image(filename=png))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb76ecb",
   "metadata": {},
   "source": [
    "## 5) Experiment B — **Thought vs Text**\n",
    "\n",
    "Protocol (pp. 20–22; Appendix pp. 48–51): inject an unrelated word over the sentence tokens, then ask the model (a) **what word it thinks about** (judge “YES”), and (b) to **repeat the sentence exactly** (string match)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98fd4dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Run thought vs text at a single layer\n",
    "TVT_LAYER = LAYER_2_3 if LAYER_2_3 is not None else 12  #@param {type:\"integer\"}\n",
    "TVT_STRENGTH = 2                                        #@param {type:\"integer\"}\n",
    "\n",
    "args = [\n",
    "    \"--model\", HF_MODEL,\n",
    "    \"--layer\", TVT_LAYER,\n",
    "    \"--strength\", TVT_STRENGTH,\n",
    "    \"--n-trials\", N_TRIALS_TVT,\n",
    "    \"--judge-provider\", JUDGE_PROVIDER,\n",
    "    \"--judge-model\", JUDGE_MODEL\n",
    "]\n",
    "if LOAD_IN_4BIT: args.append(\"--load-in-4bit\")\n",
    "if LOAD_IN_8BIT: args.append(\"--load-in-8bit\")\n",
    "if DTYPE and DTYPE != \"auto\": args += [\"--dtype\", DTYPE]\n",
    "\n",
    "run_module(\"introspect_repro.experiments.thought_vs_text\", args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f340892c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Plot thought vs text — layer‑wise lines (use latest run folder)\n",
    "TVT_RUN = latest_run_dir(\"thought_vs_text\")\n",
    "png = os.path.join(TVT_RUN, f\"tvt_layerwise_strength{TVT_STRENGTH}.png\")\n",
    "\n",
    "run_module(\"introspect_repro.plotting.plot_thought_vs_text\",\n",
    "           [\"--run-dir\", TVT_RUN, \"--strength\", TVT_STRENGTH, \"--save\", png])\n",
    "\n",
    "from IPython.display import Image, display\n",
    "display(Image(filename=png))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a23452",
   "metadata": {},
   "source": [
    "## 6) Experiment C — **Prefill intention** (apology rate)\n",
    "\n",
    "Protocol (pp. 22–25; Appendix pp. 53–55): prefill an **unrelated word**, ask whether it was intended; then **retroactively inject** the concept corresponding to the prefilled word **prior** to the prefill. Judge **apology vs intended**; plot **apology rate** vs layer (lower is better). Peak is typically **earlier** than the ~2/3 layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca1ea5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Run prefill intention\n",
    "PREFILL_LAYER = max(0, int(round(0.55 * (NUM_LAYERS-1)))) if NUM_LAYERS else 12  #@param {type:\"integer\"}\n",
    "PREFILL_STRENGTH = 4                                                               #@param {type:\"integer\"}\n",
    "\n",
    "args = [\n",
    "    \"--model\", HF_MODEL,\n",
    "    \"--layer\", PREFILL_LAYER,\n",
    "    \"--strength\", PREFILL_STRENGTH,\n",
    "    \"--n-trials\", N_TRIALS_PREFILL,\n",
    "    \"--judge-provider\", JUDGE_PROVIDER,\n",
    "    \"--judge-model\", JUDGE_MODEL\n",
    "]\n",
    "if LOAD_IN_4BIT: args.append(\"--load-in-4bit\")\n",
    "if LOAD_IN_8BIT: args.append(\"--load-in-8bit\")\n",
    "if DTYPE and DTYPE != \"auto\": args += [\"--dtype\", DTYPE]\n",
    "\n",
    "run_module(\"introspect_repro.experiments.prefill_intention\", args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb50b4fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Plot prefill intention — apology rate vs layer\n",
    "PREFILL_RUN = latest_run_dir(\"prefill_intention\")\n",
    "png = os.path.join(PREFILL_RUN, f\"prefill_apology_layerwise_strength{PREFILL_STRENGTH}.png\")\n",
    "\n",
    "run_module(\"introspect_repro.plotting.plot_prefill_intention\",\n",
    "           [\"--run-dir\", PREFILL_RUN, \"--strength\", PREFILL_STRENGTH, \"--save\", png])\n",
    "\n",
    "from IPython.display import Image, display\n",
    "display(Image(filename=png))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e75b31e4",
   "metadata": {},
   "source": [
    "## 7) Experiment D — **Intentional control** (think vs don’t think)\n",
    "\n",
    "Protocol (pp. 26–29; Appendix pp. 56–58): instruct **“Think about {word}”** versus **“Don’t think about {word}”**, record residuals during generation, and plot mean **cosine(model activations, concept vector)** vs layer for both cases.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d437f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Run intentional control across several layers\n",
    "LAYERS_TO_RUN = [0.50, 0.60, 0.66, 0.75, 0.85]  # as fractions of depth\n",
    "layers = [max(0, min(NUM_LAYERS-1, int(round(fr*(NUM_LAYERS-1))))) for fr in LAYERS_TO_RUN]\n",
    "\n",
    "for L in layers:\n",
    "    args = [\n",
    "        \"--model\", HF_MODEL,\n",
    "        \"--layer\", L,\n",
    "        \"--n-trials\", N_TRIALS_INT_CTRL\n",
    "    ]\n",
    "    if LOAD_IN_4BIT: args.append(\"--load-in-4bit\")\n",
    "    if LOAD_IN_8BIT: args.append(\"--load-in-8bit\")\n",
    "    if DTYPE and DTYPE != \"auto\": args += [\"--dtype\", DTYPE]\n",
    "\n",
    "    print(\"\\n=== Intentional control at layer\", L, \"===\")\n",
    "    run_module(\"introspect_repro.experiments.intentional_control\", args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "949202db",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Plot intentional control — mean cosine vs layer\n",
    "INTC_RUN = latest_run_dir(\"intentional_control\")\n",
    "png = os.path.join(INTC_RUN, \"intent_control_layerwise.png\")\n",
    "\n",
    "run_module(\"introspect_repro.plotting.plot_intent_control\",\n",
    "           [\"--run-dir\", INTC_RUN, \"--save\", png])\n",
    "\n",
    "from IPython.display import Image, display\n",
    "display(Image(filename=png))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1fc0d32",
   "metadata": {},
   "source": [
    "## 8) (Optional) Summarize injected‑thoughts metrics as a table\n",
    "\n",
    "The “awareness” column = **coherence ∧ affirmative ∧ correct identification**, matching the Appendix grader criteria (pp. 39–42).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "317fdeca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Build a summary table\n",
    "import json, os, glob\n",
    "import pandas as pd\n",
    "\n",
    "from introspect_repro.plotting.utils import load_results\n",
    "\n",
    "INJ = latest_run_dir(\"injected_thoughts\")\n",
    "rows = []\n",
    "for (layer, strength, j, f) in load_results(INJ):\n",
    "    trials = j[\"trials\"]\n",
    "    coh = [t.get(\"coherence\") for t in trials]\n",
    "    aff = [t.get(\"affirmative\") for t in trials]\n",
    "    cor = [t.get(\"correct_identification\") for t in trials]\n",
    "    aware = [ (t.get(\"coherence\") and t.get(\"affirmative\") and t.get(\"correct_identification\")) for t in trials ]\n",
    "    rows.append(dict(layer=layer, strength=strength,\n",
    "                     n=len(trials),\n",
    "                     coherence=sum(1 for x in coh if x)/len(coh) if coh else 0.0,\n",
    "                     affirmative=sum(1 for x in aff if x)/len(aff) if aff else 0.0,\n",
    "                     correct_id=sum(1 for x in cor if x)/len(cor) if cor else 0.0,\n",
    "                     awareness=sum(1 for x in aware if x)/len(aware) if aware else 0.0))\n",
    "df = pd.DataFrame(rows).sort_values([\"strength\",\"layer\"])\n",
    "df.style.format({c:\"{:.3f}\" for c in [\"coherence\",\"affirmative\",\"correct_id\",\"awareness\"]})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91477715",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Tips & troubleshooting\n",
    "\n",
    "- **VRAM errors** → switch to `LOAD_IN_4BIT=True`, set `DTYPE=\"bfloat16\"`, reduce `N_TRIALS_*`.  \n",
    "- **Judge time/cost** → use OpenAI `gpt-4o-mini` or Anthropic `haiku`-class as judges for quick passes.  \n",
    "- **Peaks by layer** → injected‑thoughts & thought‑vs‑text often peak near **~2/3 depth**; prefill sometimes peaks **earlier** (p. 24).  \n",
    "- **High strengths** → may cause incoherence/“brain damage” (pp. 13–14); stick to {1,2,4,8}.\n",
    "\n",
    "**Safety**: The Appendix includes a control prompt mentioning *donating to terrorist organizations*; the harness leaves that variant out by default.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch (CUDA)",
   "language": "python",
   "name": "torch-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
