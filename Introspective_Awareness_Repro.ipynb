{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00314f81",
   "metadata": {},
   "source": [
    "# Reproducing *Emergent Introspective Awareness in LLMs* — End‑to‑End Notebook\n",
    "\n",
    "This notebook runs the four experiments from the paper and recreates the **layer‑wise** plots using the harness you downloaded.\n",
    "\n",
    "**What this notebook does**  \n",
    "1. Unpacks/sets up the harness (or uses an existing `src/` in this working directory).  \n",
    "2. Installs dependencies.  \n",
    "3. Lets you choose a model and runtime (4‑bit/8‑bit/FP16).  \n",
    "4. Runs:\n",
    "   - **Injected thoughts** (layer/strength sweep)\n",
    "   - **Thought vs text**\n",
    "   - **Prefill intention**\n",
    "   - **Intentional control**\n",
    "5. Produces **layer‑wise plots** that match the paper’s figures.\n",
    "\n",
    "**Sources from the paper** (Appendix) this notebook aligns to:\n",
    "- Concept vectors on the *final `:` token* in `Human: Tell me about {word}\\nAssistant:` and subtracting the mean baseline over 100 words (pp. **36–37**).  \n",
    "- Injection windows and experiment prompts (pp. **38–42**, **48–58**).  \n",
    "- Layer‑wise lines and typical peaks (pp. **15**, **21**, **24**, **28–29**).\n",
    "\n",
    "> Note: Exact **numbers** may differ (you’re using open‑source HF models; the paper used Anthropic internal models), but you should recover the **qualitative phenomena** and **layer/strength** trends.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ec3ff3",
   "metadata": {},
   "source": [
    "## 0) Get the code into this working directory\n",
    "\n",
    "- If you have the ZIP handy (e.g., `introspection_repro_with_plots.zip`), run the cell below to unpack it.  \n",
    "- If you already have a `src/` folder here, you can skip unpacking.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d53b2685",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Unpack the harness zip (edit ZIP_PATH if needed)\n",
    "import os, zipfile, glob\n",
    "\n",
    "# Change this to the name/path of the ZIP you uploaded to this notebook runtime\n",
    "ZIP_PATH = 'introspection_repro_with_plots.zip'  # or 'introspection_repro.zip'\n",
    "\n",
    "if os.path.isdir('src'):\n",
    "    print('Found ./src — skipping unzip.')\n",
    "elif os.path.exists(ZIP_PATH):\n",
    "    with zipfile.ZipFile(ZIP_PATH, 'r') as z:\n",
    "        z.extractall('.')\n",
    "    print('Unzipped:', ZIP_PATH)\n",
    "    print('Top-level entries:', os.listdir('.')[:10])\n",
    "else:\n",
    "    print('No ./src and no ZIP_PATH found. Please upload the ZIP to this directory or clone the repo here.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f965f1",
   "metadata": {},
   "source": [
    "## 1) Install dependencies\n",
    "\n",
    "- Make sure your CUDA/Torch versions match your GPU drivers.\n",
    "- If you already have Torch installed, you can skip reinstalling it.\n",
    "\n",
    "> **Tip:** For CUDA 12.1+ wheels on Linux: `pip install torch --index-url https://download.pytorch.org/whl/cu121`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a940f3ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Install Python packages (uncomment Torch line if needed)\n",
    "# !pip install --upgrade pip\n",
    "# If you need a matching Torch build, uncomment ONE of these lines:\n",
    "# !pip install torch --index-url https://download.pytorch.org/whl/cu121   # CUDA 12.1\n",
    "# !pip install torch --index-url https://download.pytorch.org/whl/cu118   # CUDA 11.8\n",
    "\n",
    "# Install the experiment harness deps\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f705f819",
   "metadata": {},
   "source": [
    "## 2) Wire up API keys for the **LLM grader**\n",
    "\n",
    "The experiments use an **LLM judge** (Anthropic / OpenAI / OpenRouter) with the paper’s grading prompts (Appendix pp. 39–42, 54).  \n",
    "Set any one (or more) of these in the environment for this notebook process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a811a702",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Load environment variables from the .env file\n",
    "from introspect_repro import load_project_env\n",
    "\n",
    "load_project_env()\n",
    "print(\"Loaded variables from .env (existing environment values are preserved).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "934cbe06",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Set API keys for grading (fill values or leave to inherit from your environment)\n",
    "import os\n",
    "\n",
    "# Fill in only the ones you plan to use. Leaving blank keeps the existing environment value.\n",
    "ANTHROPIC = \"\"  # e.g., \"sk-ant-...\"\n",
    "OPENAI = \"\"     # e.g., \"sk-...\"\n",
    "OPENROUTER = \"\" # e.g., \"or-...\"\n",
    "\n",
    "if ANTHROPIC: os.environ['ANTHROPIC_API_KEY'] = ANTHROPIC\n",
    "if OPENAI:    os.environ['OPENAI_API_KEY'] = OPENAI\n",
    "if OPENROUTER:os.environ['OPENROUTER_API_KEY'] = OPENROUTER\n",
    "\n",
    "print(\"Anthropic:\", \"set\" if os.environ.get(\"ANTHROPIC_API_KEY\") else \"not set\")\n",
    "print(\"OpenAI   :\", \"set\" if os.environ.get(\"OPENAI_API_KEY\") else \"not set\")\n",
    "print(\"OpenRouter:\", \"set\" if os.environ.get(\"OPENROUTER_API_KEY\") else \"not set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de38bde",
   "metadata": {},
   "source": [
    "## 3) Configure model and runtime\n",
    "\n",
    "- **HF model**: any decoder‑only chat/instruct model with access to internals (e.g., Llama‑3‑8B‑Instruct, Qwen‑2.5‑7B/14B, Mixtral).  \n",
    "- **Precision**: FP16 if you have VRAM; otherwise 8‑bit/4‑bit.  \n",
    "- **Judge**: pick provider/model for grading.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83fa7c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Choose your HF model and judge\n",
    "HF_MODEL = \"meta-llama/Meta-Llama-3-8B-Instruct\"  #@param {type:\"string\"}\n",
    "LOAD_IN_4BIT = True                                #@param {type:\"boolean\"}\n",
    "LOAD_IN_8BIT = False                               #@param {type:\"boolean\"}\n",
    "DTYPE = \"bfloat16\"                                  #@param [\"bfloat16\",\"float16\",\"auto\"]\n",
    "\n",
    "JUDGE_PROVIDER = \"openai\"                           #@param [\"openai\",\"anthropic\",\"openrouter\"]\n",
    "JUDGE_MODEL = \"gpt-4o-mini\"                         #@param {type:\"string\"}\n",
    "\n",
    "N_TRIALS_INJECTED = 30                              #@param {type:\"integer\"}\n",
    "N_TRIALS_TVT = 50                                   #@param {type:\"integer\"}\n",
    "N_TRIALS_PREFILL = 50                               #@param {type:\"integer\"}\n",
    "N_TRIALS_INT_CTRL = 16                              #@param {type:\"integer\"}\n",
    "\n",
    "print(\"Model:\", HF_MODEL)\n",
    "print(\"Judge:\", JUDGE_PROVIDER, \"/\", JUDGE_MODEL)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd64e83",
   "metadata": {},
   "source": [
    "### Add the harness to `sys.path` and compute a good default layer\n",
    "\n",
    "We avoid loading full weights just to count layers by reading the config.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612e1436",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Prepare imports and compute a 2/3‑depth layer guess\n",
    "import os, sys, glob, math, subprocess, shlex, json, pathlib, time\n",
    "from IPython.display import display, Image\n",
    "\n",
    "# Add ./src to import path\n",
    "SRC_CANDIDATES = [\"./src\", \"../src\", \"/workspace/src\"]\n",
    "for c in SRC_CANDIDATES:\n",
    "    if os.path.isdir(c) and os.path.abspath(c) not in sys.path:\n",
    "        sys.path.insert(0, os.path.abspath(c))\n",
    "\n",
    "# Light‑weight: use AutoConfig to count layers\n",
    "from transformers import AutoConfig\n",
    "cfg = AutoConfig.from_pretrained(HF_MODEL)\n",
    "NUM_LAYERS = getattr(cfg, \"num_hidden_layers\", None) or getattr(cfg, \"n_layer\", None)\n",
    "LAYER_2_3 = int(round(0.66 * (NUM_LAYERS - 1))) if NUM_LAYERS else None\n",
    "print(\"num_hidden_layers:\", NUM_LAYERS, \"|  ~2/3 layer:\", LAYER_2_3)\n",
    "\n",
    "# Convenience to run a python module from this kernel\n",
    "def run_module(modname, args_list):\n",
    "    cmd = [sys.executable, \"-m\", modname] + list(map(str, args_list))\n",
    "    print(\">>>\", \" \".join(cmd))\n",
    "    subprocess.run(cmd, check=True)\n",
    "\n",
    "def latest_run_dir(exp_name):\n",
    "    cands = glob.glob(os.path.join(\"runs\", \"*\", exp_name))\n",
    "    return max(cands, key=os.path.getmtime) if cands else None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e7e7212",
   "metadata": {},
   "source": [
    "## 4) Experiment A — **Injected thoughts** (layer & strength sweeps)\n",
    "\n",
    "Protocol (Appendix pp. 36–42): compute concept vectors on the `Assistant:` colon token, subtract baseline mean (100 words), then inject on the Assistant turn of the **“injected thoughts”** prompt and judge **coherence ∧ affirmative ∧ correct identification (before saying the word)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac2d365",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Run injected thoughts (layer/strength sweep)\n",
    "LAYER_FRACTIONS = [0.60, 0.66, 0.70, 0.75, 0.80]  #@param\n",
    "STRENGTHS = [1,2,4]                               #@param\n",
    "\n",
    "args = [\n",
    "    \"--model\", HF_MODEL,\n",
    "    \"--judge-provider\", JUDGE_PROVIDER,\n",
    "    \"--judge-model\", JUDGE_MODEL,\n",
    "    \"--n-trials\", N_TRIALS_INJECTED,\n",
    "    \"--sweep-layers\", *LAYER_FRACTIONS,\n",
    "    \"--strengths\", *STRENGTHS\n",
    "]\n",
    "if LOAD_IN_4BIT: args.append(\"--load-in-4bit\")\n",
    "if LOAD_IN_8BIT: args.append(\"--load-in-8bit\")\n",
    "if DTYPE and DTYPE != \"auto\": args += [\"--dtype\", DTYPE]\n",
    "\n",
    "run_module(\"introspect_repro.experiments.injected_thoughts\", args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9904dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Plot injected thoughts: layer‑wise lines\n",
    "INJECTED_RUN = latest_run_dir(\"injected_thoughts\")\n",
    "print(\"Run dir:\", INJECTED_RUN)\n",
    "\n",
    "# Pick one strength to match the paper’s typical layer‑wise figure (strength≈2)\n",
    "PLOT_STRENGTH = 2\n",
    "png = os.path.join(INJECTED_RUN, f\"layerwise_strength{PLOT_STRENGTH}.png\")\n",
    "\n",
    "run_module(\"introspect_repro.plotting.plot_injected_thoughts\",\n",
    "           [\"--run-dir\", INJECTED_RUN, \"--strength\", PLOT_STRENGTH, \"--save\", png])\n",
    "\n",
    "display(Image(filename=png))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb76ecb",
   "metadata": {},
   "source": [
    "## 5) Experiment B — **Thought vs Text**\n",
    "\n",
    "Protocol (pp. 20–22; Appendix pp. 48–51): inject an unrelated word over the sentence tokens, then ask the model (a) **what word it thinks about** (judge “YES”), and (b) to **repeat the sentence exactly** (string match)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98fd4dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Run thought vs text at a single layer\n",
    "TVT_LAYER = LAYER_2_3 if LAYER_2_3 is not None else 12  #@param {type:\"integer\"}\n",
    "TVT_STRENGTH = 2                                        #@param {type:\"integer\"}\n",
    "\n",
    "args = [\n",
    "    \"--model\", HF_MODEL,\n",
    "    \"--layer\", TVT_LAYER,\n",
    "    \"--strength\", TVT_STRENGTH,\n",
    "    \"--n-trials\", N_TRIALS_TVT,\n",
    "    \"--judge-provider\", JUDGE_PROVIDER,\n",
    "    \"--judge-model\", JUDGE_MODEL\n",
    "]\n",
    "if LOAD_IN_4BIT: args.append(\"--load-in-4bit\")\n",
    "if LOAD_IN_8BIT: args.append(\"--load-in-8bit\")\n",
    "if DTYPE and DTYPE != \"auto\": args += [\"--dtype\", DTYPE]\n",
    "\n",
    "run_module(\"introspect_repro.experiments.thought_vs_text\", args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f340892c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Plot thought vs text — layer‑wise lines (use latest run folder)\n",
    "TVT_RUN = latest_run_dir(\"thought_vs_text\")\n",
    "png = os.path.join(TVT_RUN, f\"tvt_layerwise_strength{TVT_STRENGTH}.png\")\n",
    "\n",
    "run_module(\"introspect_repro.plotting.plot_thought_vs_text\",\n",
    "           [\"--run-dir\", TVT_RUN, \"--strength\", TVT_STRENGTH, \"--save\", png])\n",
    "\n",
    "from IPython.display import Image, display\n",
    "display(Image(filename=png))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a23452",
   "metadata": {},
   "source": [
    "## 6) Experiment C — **Prefill intention** (apology rate)\n",
    "\n",
    "Protocol (pp. 22–25; Appendix pp. 53–55): prefill an **unrelated word**, ask whether it was intended; then **retroactively inject** the concept corresponding to the prefilled word **prior** to the prefill. Judge **apology vs intended**; plot **apology rate** vs layer (lower is better). Peak is typically **earlier** than the ~2/3 layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca1ea5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Run prefill intention\n",
    "PREFILL_LAYER = max(0, int(round(0.55 * (NUM_LAYERS-1)))) if NUM_LAYERS else 12  #@param {type:\"integer\"}\n",
    "PREFILL_STRENGTH = 4                                                               #@param {type:\"integer\"}\n",
    "\n",
    "args = [\n",
    "    \"--model\", HF_MODEL,\n",
    "    \"--layer\", PREFILL_LAYER,\n",
    "    \"--strength\", PREFILL_STRENGTH,\n",
    "    \"--n-trials\", N_TRIALS_PREFILL,\n",
    "    \"--judge-provider\", JUDGE_PROVIDER,\n",
    "    \"--judge-model\", JUDGE_MODEL\n",
    "]\n",
    "if LOAD_IN_4BIT: args.append(\"--load-in-4bit\")\n",
    "if LOAD_IN_8BIT: args.append(\"--load-in-8bit\")\n",
    "if DTYPE and DTYPE != \"auto\": args += [\"--dtype\", DTYPE]\n",
    "\n",
    "run_module(\"introspect_repro.experiments.prefill_intention\", args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb50b4fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Plot prefill intention — apology rate vs layer\n",
    "PREFILL_RUN = latest_run_dir(\"prefill_intention\")\n",
    "png = os.path.join(PREFILL_RUN, f\"prefill_apology_layerwise_strength{PREFILL_STRENGTH}.png\")\n",
    "\n",
    "run_module(\"introspect_repro.plotting.plot_prefill_intention\",\n",
    "           [\"--run-dir\", PREFILL_RUN, \"--strength\", PREFILL_STRENGTH, \"--save\", png])\n",
    "\n",
    "from IPython.display import Image, display\n",
    "display(Image(filename=png))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e75b31e4",
   "metadata": {},
   "source": [
    "## 7) Experiment D — **Intentional control** (think vs don’t think)\n",
    "\n",
    "Protocol (pp. 26–29; Appendix pp. 56–58): instruct **“Think about {word}”** versus **“Don’t think about {word}”**, record residuals during generation, and plot mean **cosine(model activations, concept vector)** vs layer for both cases.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d437f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Run intentional control across several layers\n",
    "LAYERS_TO_RUN = [0.50, 0.60, 0.66, 0.75, 0.85]  # as fractions of depth\n",
    "layers = [max(0, min(NUM_LAYERS-1, int(round(fr*(NUM_LAYERS-1))))) for fr in LAYERS_TO_RUN]\n",
    "\n",
    "for L in layers:\n",
    "    args = [\n",
    "        \"--model\", HF_MODEL,\n",
    "        \"--layer\", L,\n",
    "        \"--n-trials\", N_TRIALS_INT_CTRL\n",
    "    ]\n",
    "    if LOAD_IN_4BIT: args.append(\"--load-in-4bit\")\n",
    "    if LOAD_IN_8BIT: args.append(\"--load-in-8bit\")\n",
    "    if DTYPE and DTYPE != \"auto\": args += [\"--dtype\", DTYPE]\n",
    "\n",
    "    print(\"\\n=== Intentional control at layer\", L, \"===\")\n",
    "    run_module(\"introspect_repro.experiments.intentional_control\", args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "949202db",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Plot intentional control — mean cosine vs layer\n",
    "INTC_RUN = latest_run_dir(\"intentional_control\")\n",
    "png = os.path.join(INTC_RUN, \"intent_control_layerwise.png\")\n",
    "\n",
    "run_module(\"introspect_repro.plotting.plot_intent_control\",\n",
    "           [\"--run-dir\", INTC_RUN, \"--save\", png])\n",
    "\n",
    "from IPython.display import Image, display\n",
    "display(Image(filename=png))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1fc0d32",
   "metadata": {},
   "source": [
    "## 8) (Optional) Summarize injected‑thoughts metrics as a table\n",
    "\n",
    "The “awareness” column = **coherence ∧ affirmative ∧ correct identification**, matching the Appendix grader criteria (pp. 39–42).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "317fdeca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Build a summary table\n",
    "import json, os, glob\n",
    "import pandas as pd\n",
    "\n",
    "from introspect_repro.plotting.utils import load_results\n",
    "\n",
    "INJ = latest_run_dir(\"injected_thoughts\")\n",
    "rows = []\n",
    "for (layer, strength, j, f) in load_results(INJ):\n",
    "    trials = j[\"trials\"]\n",
    "    coh = [t.get(\"coherence\") for t in trials]\n",
    "    aff = [t.get(\"affirmative\") for t in trials]\n",
    "    cor = [t.get(\"correct_identification\") for t in trials]\n",
    "    aware = [ (t.get(\"coherence\") and t.get(\"affirmative\") and t.get(\"correct_identification\")) for t in trials ]\n",
    "    rows.append(dict(layer=layer, strength=strength,\n",
    "                     n=len(trials),\n",
    "                     coherence=sum(1 for x in coh if x)/len(coh) if coh else 0.0,\n",
    "                     affirmative=sum(1 for x in aff if x)/len(aff) if aff else 0.0,\n",
    "                     correct_id=sum(1 for x in cor if x)/len(cor) if cor else 0.0,\n",
    "                     awareness=sum(1 for x in aware if x)/len(aware) if aware else 0.0))\n",
    "df = pd.DataFrame(rows).sort_values([\"strength\",\"layer\"])\n",
    "df.style.format({c:\"{:.3f}\" for c in [\"coherence\",\"affirmative\",\"correct_id\",\"awareness\"]})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91477715",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Tips & troubleshooting\n",
    "\n",
    "- **VRAM errors** → switch to `LOAD_IN_4BIT=True`, set `DTYPE=\"bfloat16\"`, reduce `N_TRIALS_*`.  \n",
    "- **Judge time/cost** → use OpenAI `gpt-4o-mini` or Anthropic `haiku`-class as judges for quick passes.  \n",
    "- **Peaks by layer** → injected‑thoughts & thought‑vs‑text often peak near **~2/3 depth**; prefill sometimes peaks **earlier** (p. 24).  \n",
    "- **High strengths** → may cause incoherence/“brain damage” (pp. 13–14); stick to {1,2,4,8}.\n",
    "\n",
    "**Safety**: The Appendix includes a control prompt mentioning *donating to terrorist organizations*; the harness leaves that variant out by default.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
